# -*- coding: utf-8 -*-
"""tensorflow_cookbook15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hkxa7CCecTLix3hWVedF0HjbDO9D1FdT

## Reading Tutorials
##### [Simple audio recognition: Recognizing keywords](https://www.tensorflow.org/tutorials/audio/simple_audio)
"""

import os,pathlib,h5py,pylab as pl,seaborn as sb
import numpy as np,pandas as pd,tensorflow as tf
from tensorflow.keras.layers.experimental import preprocessing as tkp
import tensorflow.keras.layers as tkl,tensorflow.keras.models as tkm
from IPython import display
rs=42; tf.random.set_seed(rs); np.random.seed(rs)

datadir=pathlib.Path('data/mini_speech_commands')
if not datadir.exists():
    tf.keras.utils.get_file(
        'mini_speech_commands.zip',
        origin='http://storage.googleapis.com/download.tensorflow.org/'+\
               'data/mini_speech_commands.zip',
        extract=True,cache_dir='.',cache_subdir='data')

names=np.array(tf.io.gfile.listdir(str(datadir)))
names=names[names!='README.md']
print('command names: ',names)

files=tf.io.gfile.glob(str(datadir)+'/*/*')
files=tf.random.shuffle(files)
num_samples=len(files)
print('number of total examples:',num_samples)
print('number of examples per label:',
      len(tf.io.gfile.listdir(str(datadir/names[0]))))
print('examples of the file tensor:',files[:10])

train_files=files[:int(.8*num_samples)]
valid_files=files[int(.8*num_samples):int(.9*num_samples)]
test_files=files[-int(.1*num_samples):]

print('training set size',len(train_files))
print('validation set size',len(valid_files))
print('testing set size',len(test_files))

def get_audio(file_path):
    audio,_=tf.audio.decode_wav(file_path)
    return tf.squeeze(audio,axis=-1)
def get_label(file_path):
    file_path=tf.strings.split(file_path,os.path.sep)
    return file_path[-2]
def get_audio_and_label(file_path):
    audio_binary=tf.io.read_file(file_path)
    audio=get_audio(audio_binary)
    label=get_label(file_path)
    return audio,label
get_label(files[0]).numpy()

AUTOTUNE=tf.data.AUTOTUNE
train_files_ds=tf.data.Dataset.from_tensor_slices(train_files)
train_audio_ds=train_files_ds.map(
    get_audio_and_label,num_parallel_calls=AUTOTUNE)

rows,cols=2,5
fig,axes=pl.subplots(rows,cols,figsize=(12,6))
for i,(audio,label) in enumerate(train_audio_ds.take(rows*cols)):
    ax=axes[i//cols][i%cols]
    ax.plot(audio.numpy(),color='slategray',alpha=.7)
    ax.set_yticks(np.arange(-1.2,1.2,.2))
    label=label.numpy().decode('utf-8')
    ax.set_title('$\mathbb{'+label+'}$',
                 color='slategray',fontsize='xx-large')
pl.show()

def get_spectrogram(audio,num=16000,frame_length=255,frame_step=128):
    zeros=tf.zeros([num]-tf.shape(audio),dtype=tf.float32)
    audio=tf.cast(audio,tf.float32)
    equal_length=tf.concat([audio,zeros],0)
    # a Fourier transform with time information
    spectrogram=tf.signal.stft(
        equal_length,frame_length=frame_length,frame_step=frame_step)
    # the magnitude
    spectrogram=tf.abs(spectrogram)
    return spectrogram

for audio,label in train_audio_ds.take(1):
    label=label.numpy().decode('utf-8')
    spectrogram=get_spectrogram(audio)
print('label: ',label)
print('shape of audio tensors: ',audio.shape)
print('spectrogram shape: ',spectrogram.shape)
print('audio playback: ')
display.display(display.Audio(audio,rate=16000))

def plot_spectrogram(spectrogram,ax):
    log_spec=np.log(spectrogram.T)
    height=log_spec.shape[0]
    width=log_spec.shape[1]
    x=np.linspace(0,np.size(spectrogram),num=width,dtype=int)
    y=range(height)
    ax.pcolormesh(x,y,log_spec,cmap='bone')
fig,axes=pl.subplots(2,figsize=(12,6))
timescale=np.arange(audio.shape[0])
axes[0].plot(timescale,audio.numpy(),alpha=.7,color='slategray')
axes[0].set_title('$\mathbb{'+'waveform'+'}$',
                  color='slategray',fontsize='xx-large')
axes[0].set_xlim([0,16000])
plot_spectrogram(spectrogram.numpy(),axes[1])
axes[1].set_title('$\mathbb{'+'spectrogram'+'}$',
                  color='slategray',fontsize='xx-large')
pl.show()

def get_spectrogram_and_label_id(audio,label):
    spectrogram=get_spectrogram(audio)
    spectrogram=tf.expand_dims(spectrogram,-1)
    label_id=tf.argmax(label==names)
    return spectrogram,label_id

def preprocess_ds(files):
  files_ds=tf.data.Dataset.from_tensor_slices(files)
  output_ds=files_ds.map(
      get_audio_and_label,num_parallel_calls=AUTOTUNE)
  output_ds=output_ds.map(
      get_spectrogram_and_label_id,num_parallel_calls=AUTOTUNE)
  return output_ds

train_ds=preprocess_ds(train_files)
valid_ds=preprocess_ds(valid_files)
test_ds=preprocess_ds(test_files)

rows,cols=3,5
fig,axes=pl.subplots(rows,cols,figsize=(12,9))
for i,(spectrogram,label_id) in enumerate(train_ds.take(rows*cols)):
    ax=axes[i//cols][i%cols]
    plot_spectrogram(np.squeeze(spectrogram.numpy()+.1**10),ax)
    ax.set_title('$\mathbb{'+names[label_id.numpy()]+'}$',
                 color='slategray',fontsize='xx-large')
    ax.axis('off')
pl.show()

def get_arrays(file_list):
    spectr_arr,label_arr=[],[]
    for i in range(len(file_list)):
        audio,label=get_audio_and_label(file_list[int(i)])
        spectrogram,label_id=\
        get_spectrogram_and_label_id(audio,label)
        spectr_arr.append(spectrogram.numpy())
        label_arr.append(label_id.numpy())
    return np.array(spectr_arr),np.array(label_arr,dtype='int32')
spectr_train,label_train=get_arrays(train_files)
spectr_valid,label_valid=get_arrays(valid_files)
spectr_test,label_test=get_arrays(test_files)

print(spectr_train.shape,spectr_train.dtype,
      label_train.shape,label_train.dtype,
      spectr_valid.shape,spectr_valid.dtype,
      label_valid.shape,label_valid.dtype,
      spectr_test.shape,spectr_test.dtype,
      label_test.shape,label_test.dtype)

rows,cols=3,5
fig,axes=pl.subplots(rows,cols,figsize=(12,9))
for i in range(rows*cols):
    ax=axes[i//cols][i%cols]
    plot_spectrogram(np.squeeze(spectr_train[i]+.1**10),ax)
    ax.set_title('$\mathbb{'+names[label_train[i]]+'}$',
                 color='slategray',fontsize='xx-large')
    ax.axis('off')
pl.show()

h5f='SpectrCommands124129.h5'
with h5py.File(h5f,'w') as f:
    f.create_dataset('spectr_train',data=spectr_train,
                     compression='gzip')
    f.create_dataset('label_train',data=label_train,
                     compression='gzip')
    f.create_dataset('spectr_valid',data=spectr_valid,
                     compression='gzip')
    f.create_dataset('label_valid',data=label_valid,
                     compression='gzip')
    f.create_dataset('spectr_test',data=spectr_test,
                     compression='gzip')
    f.create_dataset('label_test',data=label_test,
                     compression='gzip')
    f.close()
print('file size: %s'%list(os.stat(h5f))[6])

for spectrogram,_ in train_ds.take(1):
  input_shape=spectrogram.shape
num_labels=len(names)
norm_layer=tkp.Normalization()
norm_layer.adapt(train_ds.map(lambda x,_:x))
model=tkm.Sequential([
    tkl.InputLayer(input_shape=input_shape),
    tkp.Resizing(32,32), 
    norm_layer,
    tkl.Conv2D(32,3,activation='relu'),
    tkl.Conv2D(96,3,activation='relu'),
    tkl.MaxPooling2D(),
    tkl.Dropout(.25),
    tkl.Flatten(),
    tkl.Dense(256,activation='relu'),
    tkl.Dropout(.5),
    tkl.Dense(num_labels),
])
model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'],
)

batch_size=64
train_ds=train_ds.batch(batch_size)
valid_ds=valid_ds.batch(batch_size)

train_ds=train_ds.cache().prefetch(AUTOTUNE)
valid_ds=valid_ds.cache().prefetch(AUTOTUNE)

epochs=10
history=model.fit(
    train_ds,validation_data=valid_ds,epochs=epochs,
    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1,patience=3))

def keras_history_plot(fit_history,fig_size,color):
    keys=list(fit_history.history.keys())
    list_history=[fit_history.history[keys[i]] for i in range(len(keys))]
    dfkeys=pd.DataFrame(list_history).T
    dfkeys.columns=keys
    fig=pl.figure(figsize=(fig_size,fig_size/1.5))
    ax1=fig.add_subplot(2,1,1)
    dfkeys.iloc[:,[int(0),int(2)]].plot(
        ax=ax1,color=['slategray',color],grid=True)
    ax2=fig.add_subplot(2,1,2)
    dfkeys.iloc[:,[int(1),int(3)]].plot(
        ax=ax2,color=['slategray',color],grid=True)
    pl.tight_layout(); pl.show()
keras_history_plot(history,12,'purple')

test_audio,test_labels=[],[]
for audio,label in test_ds:
    test_audio.append(audio.numpy())
    test_labels.append(label.numpy())
test_audio=np.array(test_audio)
test_labels=np.array(test_labels)

test_preds=np.argmax(model.predict(test_audio),axis=1)
test_acc=sum(test_preds==test_labels)/len(test_labels)
print(f'test accuracy:{test_acc:.0%}')

confusion_mtx=tf.math.confusion_matrix(test_labels,test_preds) 
pl.figure(figsize=(12,9))
sb.heatmap(confusion_mtx,xticklabels=names,yticklabels=names, 
            annot=True,fmt='g')
pl.xlabel('Predictions'); pl.ylabel('Labels')
pl.show()