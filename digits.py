# -*- coding: utf-8 -*-
"""digits.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eqk81yU_y7t6Rridkgli_C7LwPLUFvH2

# Libraries & Functions
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import tensorflow as tf
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

import time,pandas,numpy,pylab,keras,sklearn
from keras.datasets import mnist
from sklearn import datasets
from scipy import stats
pylab.style.use('seaborn-whitegrid')
import warnings; warnings.filterwarnings("ignore")

from sklearn import svm,manifold,linear_model
from sklearn.semi_supervised import label_propagation
from sklearn.linear_model import LogisticRegressionCV
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,hamming_loss
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import RadiusNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier,BernoulliRBM

from keras.models import Sequential,Model
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Conv1D,Conv2D,MaxPooling1D,MaxPooling2D
from keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D
from keras.layers.advanced_activations import PReLU,LeakyReLU
from keras.layers import Input,Dense,LSTM,Dropout
from keras.layers import BatchNormalization,Activation,Flatten

def clf_fit_score(clf,x_train,x_test,y_train,y_test):
    clf.fit(x_train,y_train)    
    y_clf_train=clf.predict(x_train)
    y_clf_test=clf.predict(x_test)        
    acc_clf_train=\
    round(accuracy_score(y_train,y_clf_train),6)
    acc_clf_test=\
    round(accuracy_score(y_test,y_clf_test),6)   
    loss_clf_train=\
    round(hamming_loss(y_train,y_clf_train),6)
    loss_clf_test=\
    round(hamming_loss(y_test,y_clf_test),6)   
    return [str(clf),y_clf_train,y_clf_test,
            acc_clf_train,acc_clf_test,
            loss_clf_train,loss_clf_test]

def history_plot(fit_history):
    pylab.figure(figsize=(12,10)); pylab.subplot(211)
    keys=list(fit_history.history.keys())[0:4]
    pylab.plot(fit_history.history[keys[0]],
               color='slategray',label='train')
    pylab.plot(fit_history.history[keys[2]],
               color='steelblue',label='valid')
    pylab.xlabel("Epochs"); pylab.ylabel("Loss")
    pylab.legend(); pylab.grid(); pylab.title('Loss Function')     
    pylab.subplot(212)
    pylab.plot(fit_history.history[keys[1]],
               color='slategray',label='train')
    pylab.plot(fit_history.history[keys[3]],
               color='steelblue',label='valid')
    pylab.xlabel("Epochs"); pylab.ylabel("Accuracy")    
    pylab.legend(); pylab.grid()
    pylab.title('Accuracy'); pylab.show()

"""# Datasets"""

# 8x8 grayscale images; labeled over 10 categories
digits=datasets.load_digits()
X,y=digits.data,digits.target
[x_train1,x_test1,y_train1,y_test1]=\
train_test_split(X,y,test_size=0.2,random_state=1)
n=4; img=numpy.zeros((10*n,10*n))
for i in range(n): 
    for j in range(n): 
      img[(10*i+1):(10*i+9),(10*j+1):(10*j+9)]=\
      X[i*n+j].reshape((8,8))
pylab.figure(figsize=(5,5))
pylab.imshow(img,cmap=pylab.cm.bone)
pylab.title('Examples of 64-dimensional digits')
pylab.xticks([]); pylab.yticks([]); pylab.show()

t0=time.time()
X_emb=manifold.TSNE(n_components=2,learning_rate=700.0)\
              .fit_transform(X)
x_min,x_max=numpy.min(X_emb,0),numpy.max(X_emb,0)
X_emb=(X_emb-x_min)/(x_max-x_min)
f,ax=pylab.subplots(1,figsize=(12,5))
pylab.axis("off")
for i in range(X_emb.shape[0]):
    pylab.text(X_emb[i,0],X_emb[i,1],str(y[i]),
               color=pylab.cm.hsv(y[i]/10.))
pylab.title("t-SNE embedding %f s"%(time.time()-t0))
pylab.show()

# 28x28 grayscale images; labeled over 10 categories
(x_train2,y_train2),(x_test2,y_test2)=mnist.load_data()
n=int(len(x_test2)/2)
x_valid2,y_valid2=x_test2[:n],y_test2[:n]
x_test2,y_test2=x_test2[n:],y_test2[n:]
c_y_train2=keras.utils.to_categorical(y_train2,10)
c_y_valid2=keras.utils.to_categorical(y_valid2,10)
c_y_test2=keras.utils.to_categorical(y_test2,10)
x_train2.shape,x_valid2.shape,x_test2.shape

fig,ax=pylab.subplots(figsize=(12,2),nrows=1,
                      ncols=5,sharex=True,sharey=True)
ax=ax.flatten()
for i in range(5):
    image=x_train2[i].reshape(28,28)
    ax[i].imshow(image,cmap=pylab.cm.bone)
ax[0].set_xticks([]); ax[0].set_yticks([])
pylab.tight_layout(); pylab.gcf()
ax[2].set_title('Examples of the 784-dimensional digits',
                fontsize=25);

t0=time.time()
X_emb=manifold.TSNE(n_components=2,learning_rate=700.0)\
              .fit_transform(x_test2.reshape(-1,784))
x_min,x_max=numpy.min(X_emb,0),numpy.max(X_emb,0)
X_emb=(X_emb-x_min)/(x_max-x_min)
f,ax=pylab.subplots(1,figsize=(12,5))
pylab.axis("off")
for i in range(X_emb.shape[0]):
    pylab.text(X_emb[i,0],X_emb[i,1],str(y_test2[i]),
               color=pylab.cm.hsv(y_test2[i]/10.))
pylab.title("t-SNE embedding %f s"%(time.time()-t0))
pylab.show()

"""# Classification 1"""

clf=[LogisticRegressionCV(solver='liblinear',
                          multi_class='ovr'),
     svm.SVC(gamma='scale',C=10.0,kernel='poly'),
     KNeighborsClassifier(),
     RadiusNeighborsClassifier(radius=30),
     RandomForestClassifier(n_estimators=64,
                            max_depth=11)]

result=[]
 for c in clf:
   result.append(clf_fit_score(c,x_train1,x_test1,
                               y_train1,y_test1))

for i in range(5):
    print(result[i][0])
    print('accuracy train/test: %s'%result[i][3:5])
    print('loss train/test: %s'%result[i][5:7])
    print(80*'=')

pylab.figure(figsize=(12,5)); n=100; x=range(n)
pylab.scatter(x,y_test1[:n],marker='*',s=600,
              color='royalblue',label='Real data')
pylab.scatter(x,result[1][2][:n],marker='v',s=200,
              color='darkblue',label='SVC')
pylab.scatter(x,result[0][2][:n],marker='s',s=100,
              color='darkgrey',label='Logistic RegressionCV')
pylab.scatter(x,result[2][2][:n],marker='o',s=50,
              color='darkgreen',label='KNeighborsClassifier')
pylab.xlabel('Observations'); pylab.ylabel('Targets')
pylab.title('Classifiers. Test Results. Digits 1')
pylab.legend(loc=2,fontsize=10); pylab.show()

nn_clf1=MLPClassifier(hidden_layer_sizes=(512,),max_iter=70,
                      solver='sgd',verbose=1,
                      random_state=1,learning_rate_init=.01)
nn_clf1.fit(x_train1,y_train1)
[nn_clf1.score(x_train1,y_train1),
 nn_clf1.score(x_test1,y_test1)]

# Commented out IPython magic to ensure Python compatibility.
print("MLPClassifier:\n%s\n"\
#       %(classification_report(y_test1,
                              nn_clf1.predict(x_test1))))

x_train1_scaled=\
(x_train1-numpy.min(x_train1,0))/(numpy.max(x_train1,0)+0.0001)
x_test1_scaled=\
(x_test1-numpy.min(x_test1,0))/(numpy.max(x_test1,0)+0.0001)
logistic=LogisticRegression(solver='liblinear',multi_class='ovr',
                            max_iter=50,tol=0.0001,C=5000.0)
brbm=BernoulliRBM(random_state=0,verbose=False)
brbm.learning_rate,brbm.n_iter,brbm.n_components=0.05,50,64
nn_clf2=Pipeline(steps=[('brbm',brbm),('logistic',logistic)])
nn_clf2.fit(x_train1_scaled,y_train1)

# Commented out IPython magic to ensure Python compatibility.
print("Logistic regression using BRBM features:\n%s\n"\
#       %(classification_report(y_test1,
                              nn_clf2.predict(x_test1_scaled))))

"""# Classification 2"""

n_total=42000; n_labeled=38000
X2=numpy.copy(x_train2.reshape(-1,784)[:n_total])
y2=numpy.copy(y_train2[:n_total]).astype('int64')
y2[n_labeled:]=-1
lp_model=label_propagation\
         .LabelSpreading(kernel='knn',n_neighbors=10,max_iter=20)
lp_model.fit(X2,y2)
predicted_labels=lp_model.transduction_[n_labeled:n_total]
true_labels=y_train2[n_labeled:n_total]

print("Label Spreading: %d labeled & %d unlabeled points (%d total)"%
      (n_labeled,n_total-n_labeled,n_total))
print(classification_report(true_labels,predicted_labels))
print("Confusion matrix")
print(confusion_matrix(true_labels,predicted_labels,
                       labels=lp_model.classes_))
predict_entropies=\
stats.distributions.entropy(lp_model.label_distributions_.T)
uncertainty_index=numpy.argsort(predict_entropies)[-10:]

fig=pylab.figure(figsize=(12,5))
for index,image_index in enumerate(uncertainty_index):
    image=x_train2[image_index]
    sub=fig.add_subplot(2,5,index+1)
    sub.imshow(image,cmap=pylab.cm.bone)
    pylab.xticks([]); pylab.yticks([])
    sub.set_title('predict: %i\ntrue: %i'%(
        lp_model.transduction_[image_index],
        y_train2[image_index]))

"""# Classification 3"""

clf=MLPClassifier(hidden_layer_sizes=(256,),
                  max_iter=7,solver='adam',
                  verbose=0,random_state=1,
                  learning_rate_init=.001)
clf.fit(x_train2.reshape(-1,784),y_train2)
print(clf.score(x_test2.reshape(-1,784),y_test2),
      clf.score(x_valid2.reshape(-1,784),y_valid2))
y_test2_predictions=clf.predict(x_test2.reshape(-1,784))
pylab.figure(figsize=(12,5))
pylab.scatter(range(100),y_test2[:100],s=100)
pylab.scatter(range(100),y_test2_predictions[:100],s=25)
pylab.show()

clf=MLPClassifier(hidden_layer_sizes=(196,),max_iter=100,alpha=1e-4,
                  solver='sgd',verbose=2, tol=1e-4,random_state=1,
                  learning_rate_init=1e-3)
clf.fit(x_train2.reshape(-1,784),y_train2)
print(clf.score(x_test2.reshape(-1,784),y_test2),
      clf.score(x_valid2.reshape(-1,784),y_valid2))
y_test2_predictions=clf.predict(x_test2.reshape(-1,784))
pylab.figure(figsize=(12,5))
pylab.scatter(range(100),y_test2[:100],s=100)
pylab.scatter(range(100),y_test2_predictions[:100],s=25)
pylab.show()

def model():
    model=Sequential()    
    model.add(Dense(32,activation='relu',input_shape=(784,)))
    model.add(Dropout(rate=0.1))    
    model.add(Dense(1024,activation='relu'))
    model.add(Dropout(rate=0.1))    
    model.add(Dense(10,activation='softmax'))
    model.compile(optimizer='adam',loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model
model=model(); fw='weights.best.model.hdf5'
early_stopping=keras.callbacks.EarlyStopping(monitor='val_loss',patience=20)
checkpointer=keras.callbacks.ModelCheckpoint(filepath=fw,save_best_only=True)
lr_reduction=keras.callbacks.ReduceLROnPlateau(monitor='val_loss',verbose=2,
                                               patience=5,factor=0.5)
history=model.fit(x_train2.reshape(-1,784),c_y_train2, 
                  validation_data=(x_valid2.reshape(-1,784),c_y_valid2),
                  epochs=100,batch_size=128,verbose=0,
                  callbacks=[early_stopping,checkpointer,lr_reduction]);

history_plot(history)

model.load_weights(fw)
y_test2_predictions=model.predict_classes(x_test2.reshape(-1,784))
pylab.figure(figsize=(12,5))
pylab.scatter(range(100),y_test2[:100],s=100)
pylab.scatter(range(100),y_test2_predictions[:100],s=25)
model.evaluate(x_test2.reshape(-1,784),c_y_test2)

def model():
    model_input=Input(shape=(28,28,1))
    x=BatchNormalization()(model_input)   
#    x=Conv2D(28,(5,5),padding='same')(x)
#    x=LeakyReLU(alpha=0.02)(x)
#    x=MaxPooling2D(pool_size=(2, 2))(x)
#    x=Dropout(0.25)(x)   
    x=Conv2D(28,(5,5),padding='same')(x)
    x=LeakyReLU(alpha=0.02)(x)
    x=MaxPooling2D(strides=(2,2))(x)
    x=Dropout(0.25)(x)    
    x=Conv2D(128,(5,5))(x)
    x=LeakyReLU(alpha=0.02)(x)
    x=MaxPooling2D(strides=(2,2))(x)
    x=Dropout(0.25)(x) 
    x=GlobalMaxPooling2D()(x)  
    x=Dense(512)(x)
    x=LeakyReLU(alpha=0.02)(x)
    x=Dropout(0.5)(x)   
    y=Dense(10,activation='softmax')(x)   
    model=Model(input=model_input,output=y)    
#    optimizer=RMSprop(lr=0.001,rho=0.9,epsilon=1e-08,decay=0.0)   
    model.compile(loss='categorical_crossentropy',
                  optimizer='nadam',metrics=['accuracy'])
    return model
model=model(); fw='weights.best.model.hdf5'
early_stopping=keras.callbacks.EarlyStopping(monitor='val_loss',
                                             verbose=2,patience=20)
checkpointer=keras.callbacks.ModelCheckpoint(filepath=fw,verbose=2,
                                             save_best_only=True)
lr_reduction=keras.callbacks.ReduceLROnPlateau(monitor='val_loss',verbose=2,
                                               patience=5,factor=.75,
                                               min_lr=.000001)
history=model.fit(x_train2.reshape(-1,28,28,1),c_y_train2, 
                  validation_data=(x_valid2.reshape(-1,28,28,1),c_y_valid2),
                  epochs=100,batch_size=128,verbose=2,
                  callbacks=[early_stopping,checkpointer,lr_reduction]);

history_plot(history)

model.load_weights(fw)
y_test2_predictions=\
model.predict(x_test2.reshape(-1,28,28,1))\
.argmax(axis=-1)
pylab.figure(figsize=(12,5))
pylab.scatter(range(100),y_test2[:100],s=100)
pylab.scatter(range(100),y_test2_predictions[:100],s=25)
model.evaluate(x_test2.reshape(-1,28,28,1),c_y_test2)

steps,epochs=1000,50
data_generator=\
ImageDataGenerator(zoom_range=0.2,shear_range=0.2, 
                   rotation_range=20,width_shift_range=0.2,
                   height_shift_range=0.2)
data_generator.fit(x_train2.reshape(-1,28,28,1))

history=model.\
fit_generator(data_generator.flow(x_train2.reshape(-1,28,28,1),
                                  c_y_train2,batch_size=128),
              steps_per_epoch=steps,epochs=epochs,verbose=2,
              validation_data=(x_valid2.reshape(-1,28,28,1),c_y_valid2), 
              callbacks=[early_stopping,checkpointer,lr_reduction])

history_plot(history)

model.load_weights(fw)
y_test2_predictions=\
model.predict(x_test2.reshape(-1,28,28,1))\
.argmax(axis=-1)
pylab.figure(figsize=(12,5))
pylab.scatter(range(100),y_test2[:100],s=100)
pylab.scatter(range(100),y_test2_predictions[:100],s=25)
model.evaluate(x_test2.reshape(-1,28,28,1),c_y_test2)

def model():
    model=Sequential()
    model.add(LSTM(196,return_sequences=True,
                   input_shape=(1,784)))    
    model.add(LSTM(196,return_sequences=True))   
    model.add(LSTM(784))      
    model.add(Dense(10, activation='softmax'))
    model.compile(loss='categorical_crossentropy', 
                  optimizer='nadam', metrics=['accuracy'])    
    return model
model=model(); fw='weights.best.model.hdf5'
early_stopping=keras.callbacks.EarlyStopping(monitor='val_loss',patience=20)
checkpointer=keras.callbacks.ModelCheckpoint(filepath=fw,save_best_only=True)
lr_reduction=keras.callbacks.ReduceLROnPlateau(monitor='val_loss',verbose=2,
                                               patience=5,factor=.75,
                                               min_lr=.000001)
history=model.fit(x_train2.reshape(-1,1,784),c_y_train2, 
                  validation_data=(x_valid2.reshape(-1,1,784),c_y_valid2),
                  epochs=100,batch_size=128,verbose=2,
                  callbacks=[early_stopping,checkpointer,lr_reduction]);

history_plot(history)

model.load_weights(fw)
y_test2_predictions=\
model.predict(x_test2.reshape(-1,1,784))\
.argmax(axis=-1)
pylab.figure(figsize=(12,5))
pylab.scatter(range(100),y_test2[:100],s=100)
pylab.scatter(range(100),y_test2_predictions[:100],s=25)
model.evaluate(x_test2.reshape(-1,1,784),c_y_test2)