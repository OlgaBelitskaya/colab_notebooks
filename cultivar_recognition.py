# -*- coding: utf-8 -*-
"""cultivar_recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pymaadPUhSm0T9N5h44ls-mAcrylfa2F

# üìë &nbsp; Deep Learning. Cultivar Recognition
<a href="https://olgabelitskaya.github.io/README.html">&#x1F300; &nbsp; 
Home Page &nbsp;</a> &nbsp; &nbsp; &nbsp;
<a href="https://www.instagram.com/olga.belitskaya/">&#x1F300; &nbsp;
Instagram Posts &nbsp;</a> &nbsp; &nbsp; &nbsp;
<a href="https://www.pinterest.ru/olga_belitskaya/code-style/">&#x1F300; &nbsp; Pinterest Posts</a><br/>
For this project, I have created the dataset of <br/>
776 color images (160x160x3) with tomato cultivars.<br/>
## ‚úíÔ∏è &nbsp; Importing Libraries and Defining Helpful Functions
"""

!pip install --upgrade neural_structured_learning --user

spath='/usr/local/lib/python3.6/dist-packages'
import sys; sys.path.append(spath)
import warnings; warnings.filterwarnings('ignore')
import h5py,urllib,zipfile,tensorflow as tf
import pandas as pd,numpy as np,pylab as pl
import tensorflow_hub as th
import neural_structured_learning as nsl

"""## ‚úíÔ∏è &nbsp; Data Loading and Preprocessing"""

fpath='https://olgabelitskaya.github.io/'
zf='TomatoCultivarImages.h5.zip'
input_file=urllib.request.urlopen(fpath+zf)
output_file=open(zf,'wb')
output_file.write(input_file.read())
output_file.close(); input_file.close()
zipf=zipfile.ZipFile(zf,'r')
zipf.extractall(''); zipf.close()
f=h5py.File(zf[:-4],'r')
keys=list(f.keys()); print(keys)

names=['Kumato','Beefsteak','Tigerella',
       'Roma','Japanese Black Trifele',
       'Yellow Pear','Sun Gold','Green Zebra',
       'Cherokee Purple','Oxheart','Blue Berries',
       'San Marzano','Banana Legs',
       'German Orange Strawberry','Supersweet 100']
n_classes=len(names)
x_test=np.array(f[keys[0]])
y_test=np.array(f[keys[1]],dtype=np.float32)
x_train=np.array(f[keys[2]])
y_train=np.array(f[keys[3]],dtype=np.float32)
fig=pl.figure(figsize=(10,4))
n=np.random.randint(1,50)
for i in range(n,n+5):
    ax=fig.add_subplot(1,5,i-n+1,\
    xticks=[],yticks=[],
    title=names[int(y_test[i])])
    ax.imshow((x_test[i]))

n=int(len(x_test)/2)
x_valid,y_valid=x_test[:n],y_test[:n]
x_test,y_test=x_test[n:],y_test[n:]
df=pd.DataFrame([[x_train.shape,x_valid.shape,x_test.shape],
                 [x_train.dtype,x_valid.dtype,x_test.dtype],
                 [y_train.shape,y_valid.shape,y_test.shape],
                 [y_train.dtype,y_valid.dtype,y_test.dtype]],
                 columns=['train','valid','test'],
                 index=["images' shape",'image type',
                        "labels' shape",'label type'])
df

"""## ‚úíÔ∏è &nbsp; Classification Models
CNN Based Models with Adversarial Regularization
"""

batch_size=64; img_size=x_train.shape[1]; epochs=30
base_model=tf.keras.Sequential([
    tf.keras.Input((img_size,img_size,3),name='input'),
    tf.keras.layers.Conv2D(32,(5,5),padding='same'),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Dropout(.25),
    tf.keras.layers.Conv2D(196,(5,5)),
    tf.keras.layers.Activation('relu'),    
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Dropout(.25),
    tf.keras.layers.GlobalMaxPooling2D(),    
    tf.keras.layers.Dense(512),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dropout(.25),
    tf.keras.layers.Dense(128),
    tf.keras.layers.Activation('relu'),
    tf.keras.layers.Dropout(.25),
    tf.keras.layers.Dense(n_classes,activation='softmax')
])
adv_config=nsl.configs\
.make_adv_reg_config(multiplier=.2,adv_step_size=.05)
adv_model=nsl.keras\
.AdversarialRegularization(base_model,adv_config=adv_config)
adv_model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

train=tf.data.Dataset.from_tensor_slices(
    {'input':x_train,'label':y_train})\
     .batch(batch_size)
valid=tf.data.Dataset.from_tensor_slices(
    {'input':x_valid,'label':y_valid})\
     .batch(batch_size)
valid_steps=x_valid.shape[0]//batch_size
adv_model.fit(train,validation_data=valid,verbose=2,
              validation_steps=valid_steps,epochs=epochs)

adv_model.evaluate({'input':x_test,'label':y_test})

"""TFHub Models"""

#y_train,y_valid,y_test=\
#np.array(y_train,dtype='int32'),\
#np.array(y_valid,dtype='int32'),\
#np.array(y_test,dtype='int32')

def premodel(pix,den,mh,lbl):
    model=tf.keras.Sequential([
        tf.keras.layers.Input((pix,pix,3),
                              name='input'),
        th.KerasLayer(mh,trainable=True),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(den,activation='relu'),
        tf.keras.layers.Dropout(rate=.5),
        tf.keras.layers.Dense(lbl,activation='softmax')])
    model.compile(optimizer='adam',metrics=['accuracy'],
                  loss='sparse_categorical_crossentropy')
    display(model.summary())
    return model
def cb(fw):
    early_stopping=tf.keras.callbacks\
    .EarlyStopping(monitor='val_loss',patience=20,verbose=2)
    checkpointer=tf.keras.callbacks\
    .ModelCheckpoint(filepath=fw,save_best_only=True,verbose=2)
    lr_reduction=tf.keras.callbacks\
    .ReduceLROnPlateau(monitor='val_loss',verbose=2,
                       patience=5,factor=.8)
    return [checkpointer,early_stopping,lr_reduction]

[handle_base,pixels]=["mobilenet_v2_075_160",160]
mhandle="https://tfhub.dev/google/imagenet/{}/feature_vector/4"\
.format(handle_base)
fw='weights.best.hdf5'

model=premodel(pixels,1024,mhandle,n_classes)
history=model.fit(x=x_train,y=y_train,batch_size=32,
                  epochs=70,callbacks=cb(fw),
                  validation_data=(x_valid,y_valid))

model.load_weights(fw)
model.evaluate(x_test,y_test)

[handle_base,pixels]=["inception_v3",160]
mhandle="https://tfhub.dev/google/imagenet/{}/classification/4"\
.format(handle_base)

model=premodel(pixels,1024,mhandle,n_classes)
history=model.fit(x=x_train,y=y_train,batch_size=32,
                  epochs=70,callbacks=cb(fw),
                  validation_data=(x_valid,y_valid))

model.load_weights(fw)
model.evaluate(x_test,y_test)