# -*- coding: utf-8 -*-
"""boston_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1atPu92jNJ-qBsFpbOiMojHGXCu6gJWYW

# üìë &nbsp; Deep Learning. Practice Project 1_0: Neural Networks for Regression
<a href='https://olgabelitskaya.gitlab.io/index.html'>&#x1F300; &nbsp; Homepage &nbsp; &nbsp;</a>
<a href='https://olgabelitskaya.gitlab.io/deep_learning_projects/index.html'>&#x1F300; &nbsp; Project List &nbsp; &nbsp;</a>
<a href='https://olgabelitskaya.github.io/README.html'>&#x1F300; &nbsp; GitHub Pages &nbsp; &nbsp;</a>
<a href='https://www.instagram.com/olga.belitskaya/'>&#x1F300; &nbsp; Instagram Posts &nbsp; &nbsp;</a>
<a href='https://www.pinterest.ru/olga_belitskaya/code-style/'>&#x1F300; &nbsp; Pinterest Posts</a><br/>

In this project, we'll evaluate the performance and predictive power of neural networks in the sphere of regression tasks. 

Models will be trained and tested on data collected from homes in suburbs of Boston, Massachusetts.

Origin: This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.

Creators: Harrison, D. and Rubinfeld, D.L.

Data Set Information: Concerns housing values in suburbs of Boston.
"""

#@title Attributes
from IPython.display import display,HTML
display(HTML("""
<style>
@import 'https://fonts.googleapis.com/css?family=Akronim|Ruthie';
</style>
<table style='width:80%; background-color:black; 
              font-family:Ruthie; font-size:200%;'>
<tr style='color:white; font-family:Akronim;'>
  <th>Attribute</th><th>Description</th></tr>
<tr><td style='color:#F898C8;'><center>CRIM</center></td>
  <td style='color:#F898C8;'>
    <left>per capita crime rate by town</left></td></tr>
<tr><td style='color:#E91E63;'><center>ZN</center></td>
  <td style='color:#E91E63;'>
    <left>proportion of residential land zoned for lots over 25,000 sq.ft.
    </left></td></tr>
<tr><td style='color:#D62518;'><center>INDUS</center></td>
  <td style='color:#D62518;'>
    <left>proportion of non-retail business acres per town
    </left></td></tr>
<tr><td style='color:#AD0000;'><center>CHAS</center></td>
  <td style='color:#AD0000;'>
    <left>Charles River dummy variable (1 if tract bounds river; 0 otherwise)
    </left></td></tr>
<tr><td style='color:#FA7A00;'><center>NOX</center></td>
  <td style='color:#FA7A00;'>
    <left>nitric oxides concentration (parts per 10 million)
    </left></td></tr> 
<tr><td style='color:#FED85D;'><center>RM</center></td>
  <td style='color:#FED85D;'>
    <left>average number of rooms per dwelling</left></td></tr> 
<tr><td style='color:#91E351;'><center>AGE</center></td>
  <td style='color:#91E351;'>
    <left>proportion of owner-occupied units built prior to 1940
    </left></td></tr> 
<tr><td style='color:#00D8A0;'><center>DIS</center></td>
  <td style='color:#00D8A0;'>
    <left>weighted distances to five Boston employment centres
    </left></td></tr> 
<tr><td style='color:#1CAC78;'><center>RAD</center></td>
  <td style='color:#1CAC78;'>
    <left>index of accessibility to radial highways</left></td></tr>
<tr><td style='color:#004C71;'><center>TAX</center></td>
  <td style='color:#004C71;'>
    <left>full-value property-tax rate per 10,000 USD</left></td></tr>
<tr><td style='color:#1AADE0;'><center>PTRATIO</center></td>
  <td style='color:#1AADE0;'>
    <left>pupil-teacher ratio by town</left></td></tr>
<tr><td style='color:#0069BD;'><center>B</center></td>
  <td style='color:#0069BD;'>
    <left>1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
    </left></td></tr>
<tr><td style='color:#333399;'><center>LSTAT</center></td>
  <td style='color:#333399;'>
    <left>% lower status of the population</left></td></tr> 
<tr><td style='color:#7851A9;'><center>MEDV</center></td>
  <td style='color:#7851A9;'>
    <left>Median value of owner-occupied homes in 1000 USD</left></td></tr>
</table>"""))

"""The Boston housing data was collected in 1978 and each of the 

506 entries represents aggregated data about 14 features for homes from various suburbs.
## ‚úíÔ∏è &nbsp;Step 0. Code Modules & Helpful Functions

"""

import sqlite3,seaborn as sn,pylab as pl
import tensorflow as tf,pandas as pd,numpy as np
from sklearn.model_selection import train_test_split
from sklearn import datasets,linear_model,svm
from sklearn.metrics import mean_squared_error,median_absolute_error,\
mean_absolute_error,r2_score,explained_variance_score
from sklearn.tree import DecisionTreeRegressor,ExtraTreeRegressor
from sklearn.ensemble import BaggingRegressor,RandomForestRegressor,\
AdaBoostRegressor,GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor,RadiusNeighborsRegressor
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,\
QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.kernel_ridge import KernelRidge
from sklearn.cross_decomposition import PLSRegression
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import WhiteKernel,RationalQuadratic,RBF
from sklearn.semi_supervised import LabelPropagation,LabelSpreading
from sklearn.isotonic import IsotonicRegression
from tensorflow.keras.datasets import boston_housing
from tensorflow.keras.callbacks import \
ModelCheckpoint,EarlyStopping,ReduceLROnPlateau
from tensorflow.keras.models import Sequential,load_model
from tensorflow.keras.layers import Dense,LSTM,\
GlobalAveragePooling1D,Activation,Flatten,Dropout,BatchNormalization,\
Conv1D,MaxPooling1D,GlobalMaxPooling1D,PReLU,LeakyReLU

def connect2db(dbf):
    connection=None
    try:
        connection=sqlite3.connect(dbf)
        if connection is not None: 
            cursor=connection.cursor()
        return connection,cursor
    except Error as err:
        print(err)
        if connection is not None: 
            connection.close()
conn,cur=connect2db('boston.db')

def history_plot(fit_history,n):
    keys=list(fit_history.history.keys())[0:4]
    pl.figure(figsize=(12,8)); pl.subplot(211)
    pl.plot(fit_history.history[keys[0]][n:],
            color='slategray',label='train')
    pl.plot(fit_history.history[keys[2]][n:],
            color='#348abd',label='valid')
    pl.xlabel('epochs'); pl.ylabel('loss')
    pl.legend(); pl.title('loss function')      
    pl.subplot(212)
    pl.plot(fit_history.history[keys[1]][n:],
            color='slategray',label='train')
    pl.plot(fit_history.history[keys[3]][n:],
            color='#348abd',label='valid')
    pl.xlabel('epochs'); pl.ylabel('mae')    
    pl.legend(); pl.title('mean absolute error')
    pl.tight_layout(); pl.show()

def predict_plot(y,y_mlp,y_cnn,y_rnn,ti):
    pl.figure(figsize=(12,6))
    pl.scatter(range(n),y[:n],marker='*',s=100,
               color='black',label='real data')
    pl.plot(y_mlp[:n],label='mlp')
    pl.plot(y_cnn[:n],label='cnn')
    pl.plot(y_rnn[:n],label='rnn')
    pl.xlabel('data points')
    pl.ylabel('predicted and real target values')
    pl.legend(); pl.title(ti); pl.show()

""" ## ‚úíÔ∏è &nbsp;Step 1. Data Loading"""

boston_data=datasets.load_boston()
columns=boston_data.feature_names
boston_df=pd.DataFrame(boston_data.data,columns=columns)
boston_df['MEDV']=boston_data.target
boston_df.to_sql('main',con=conn,if_exists='replace')
boston_df.head(7)

pearson=boston_df.corr(method='pearson')
corr_with_prices=pearson.iloc[-1][:-1]
pd.DataFrame(
    corr_with_prices[abs(corr_with_prices).argsort()[::-1]])

pd.read_sql_query('''
SELECT ZN,
       AVG(LSTAT),
       AVG(RM),
       AVG(PTRATIO),
       AVG(INDUS),
       AVG(TAX)
FROM main
GROUP BY ZN;
''',con=conn).set_index('ZN').head(7)

n=int(51)
(x_train,y_train),(x_test,y_test)=boston_housing.load_data()
x_valid,y_valid=x_test[:n],y_test[:n]
x_test,y_test=x_test[n:],y_test[n:]
pd.DataFrame([['training feature`s shape:',x_train.shape],
              ['training target`s shape:',y_train.shape],
              ['validating feature`s shape:',x_valid.shape],
              ['validating target`s shape:',y_valid.shape],
              ['testing feature`s shape:',x_test.shape],
              ['testing target`s shape',y_test.shape]])

pl.style.use('seaborn-whitegrid')
pl.figure(1,figsize=(12,4)); pl.subplot(121)
sn.histplot(y_train,color='#348abd',bins=30,kde=True)
pl.ylabel('distribution'); pl.xlabel('prices')
pl.subplot(122)
sn.histplot(np.log(y_train),color='#348abd',bins=30,kde=True)
pl.ylabel('distribution'); pl.xlabel('logarithmic prices')
pl.suptitle('<<< boston housing data >>>',fontsize=15,color='#348abd')
pl.tight_layout(2); pl.show()

"""## ‚úíÔ∏è &nbsp;Step 2. Various Neural Networks with Keras Py
<p>Multilayer Perceptron (MLP)</p><br/>
"""

def mlp_model():
    model=Sequential() 
    model.add(Dense(1024,input_dim=13))
    model.add(LeakyReLU(alpha=.025)) 
    model.add(Dense(104))     
    model.add(LeakyReLU(alpha=.025))   
    model.add(Dense(1,kernel_initializer='normal'))    
    model.compile(loss='mse',optimizer='rmsprop',metrics=['mae'])
    return model
mlp_model=mlp_model()

fp='/tmp/checkpoint'
checkpointer=ModelCheckpoint(
    filepath=fp,verbose=0,save_weights_only=True,
    monitor='val_loss',mode='min',save_best_only=True)
lr_reduction=ReduceLROnPlateau(
    monitor='val_loss',patience=10,verbose=0,factor=.75)
mlp_history=mlp_model.fit(
    x_train,y_train,batch_size=14,validation_data=(x_valid,y_valid),
    epochs=300,verbose=0,callbacks=[checkpointer,lr_reduction])

history_plot(mlp_history,2)

mlp_model.load_weights(fp)
y_train_mlp=mlp_model.predict(x_train)
y_valid_mlp=mlp_model.predict(x_valid)
y_test_mlp=mlp_model.predict(x_test)
score_train_mlp=r2_score(y_train,y_train_mlp)
score_valid_mlp=r2_score(y_valid,y_valid_mlp)
score_test_mlp=r2_score(y_test,y_test_mlp)
pd.DataFrame([['train R2 score:',score_train_mlp],
              ['valid R2 score:',score_valid_mlp],
              ['test R2 score:',score_test_mlp]])

"""#### Convolutional Neural Network (CNN)"""

def cnn_model():
    model=Sequential()       
    model.add(Conv1D(13,5,padding='valid',input_shape=(13,1)))
    model.add(LeakyReLU(alpha=.025))
    model.add(MaxPooling1D(pool_size=2))   
    model.add(Conv1D(128,3,padding='valid'))
    model.add(LeakyReLU(alpha=.025))
    model.add(MaxPooling1D(pool_size=2))   
    model.add(Flatten())      
    model.add(Dense(16,activation='relu',kernel_initializer='normal'))
    model.add(Dropout(.1))  
    model.add(Dense(1,kernel_initializer='normal'))  
    model.compile(loss='mse',optimizer='nadam',metrics=['mae'])
    return model
cnn_model=cnn_model()

checkpointer=ModelCheckpoint(
    filepath=fp,verbose=0,save_weights_only=True,
    monitor='val_loss',mode='min',save_best_only=True)
lr_reduction=ReduceLROnPlateau(
    monitor='val_loss',patience=10,verbose=0,factor=.75)
cnn_history=cnn_model.fit(
    x_train.reshape(-1,13,1),y_train, 
    validation_data=(x_valid.reshape(-1,13,1),y_valid),
    epochs=300,batch_size=14,verbose=0, 
    callbacks=[checkpointer,lr_reduction])

history_plot(cnn_history,2)

cnn_model.load_weights(fp)
y_train_cnn=cnn_model.predict(x_train.reshape(-1,13,1))
y_valid_cnn=cnn_model.predict(x_valid.reshape(-1,13,1))
y_test_cnn=cnn_model.predict(x_test.reshape(-1,13,1))
score_train_cnn=r2_score(y_train,y_train_cnn)
score_valid_cnn=r2_score(y_valid,y_valid_cnn)
score_test_cnn=r2_score(y_test,y_test_cnn)
pd.DataFrame([['train R2 score:',score_train_cnn],
              ['valid R2 score:',score_valid_cnn],
              ['test R2 score:',score_test_cnn]])

"""#### Recurrent Neural Network (RNN)"""

def rnn_model():
    model=Sequential()   
    model.add(LSTM(104,return_sequences=True,input_shape=(1,13)))
    model.add(LSTM(104,return_sequences=True))
    model.add(LSTM(104,return_sequences=False))   
    model.add(Dense(1))
    model.compile(optimizer='rmsprop',loss='mse',metrics=['mae'])       
    return model
rnn_model=rnn_model()

checkpointer=ModelCheckpoint(
    filepath=fp,verbose=0,save_weights_only=True,
    monitor='val_loss',mode='min',save_best_only=True)
lr_reduction=ReduceLROnPlateau(
    monitor='val_loss',patience=10,verbose=0,factor=.75)
rnn_history=rnn_model.fit(
    x_train.reshape(-1,1,13),y_train, 
    validation_data=(x_valid.reshape(-1,1,13),y_valid),
    epochs=300,batch_size=14,verbose=0, 
    callbacks=[checkpointer,lr_reduction])

history_plot(rnn_history,3)

rnn_model.load_weights(fp)
y_train_rnn=rnn_model.predict(x_train.reshape(-1,1,13))
y_valid_rnn=rnn_model.predict(x_valid.reshape(-1,1,13))
y_test_rnn=rnn_model.predict(x_test.reshape(-1,1,13))
score_train_rnn=r2_score(y_train,y_train_rnn)
score_valid_rnn=r2_score(y_valid,y_valid_rnn)
score_test_rnn=r2_score(y_test,y_test_rnn)
pd.DataFrame([['train R2 score:',score_train_rnn],
              ['valid R2 score:',score_valid_rnn],
              ['test R2 score:',score_test_rnn]])

"""## ‚úíÔ∏è &nbsp;Step 3. Predictions of Keras Algorithms Py"""

ti='Train Set; Neural Network Predictions vs Real Data'
predict_plot(y_train,y_train_mlp,y_train_cnn,y_train_rnn,ti)

ti='Validation Set; Neural Network Predictions vs Real Data'
predict_plot(y_valid,y_valid_mlp,y_valid_cnn,y_valid_rnn,ti)

ti='Test Set; Neural Network Predictions vs Real Data'
predict_plot(y_test,y_test_mlp,y_test_cnn,y_test_rnn,ti)

"""## ‚úíÔ∏è  Step 4. Comparing with Sklearn Algorithms Py



"""

def regressor_fit_score(
    regressor,regressor_name,dataset,x_train,x_test,y_train,y_test,n=6):
    regressor_list.append(str(regressor))
    regressor_names.append(regressor_name)
    reg_datasets.append(dataset)    
    regressor.fit(x_train,y_train)
    y_reg_train=regressor.predict(x_train)
    y_reg_test=regressor.predict(x_test)    
    r2_reg_train=round(r2_score(y_train,y_reg_train),n)
    r2_train.append(r2_reg_train)
    r2_reg_test=round(r2_score(y_test,y_reg_test),n)
    r2_test.append(r2_reg_test)    
    ev_reg_train=round(explained_variance_score(y_train,y_reg_train),n)
    ev_train.append(ev_reg_train)
    ev_reg_test=round(explained_variance_score(y_test, y_reg_test),n)
    ev_test.append(ev_reg_test)    
    mse_reg_train=round(mean_squared_error(y_train,y_reg_train),n)
    mse_train.append(mse_reg_train)
    mse_reg_test=round(mean_squared_error(y_test,y_reg_test),n)
    mse_test.append(mse_reg_test)
    mae_reg_train=round(mean_absolute_error(y_train,y_reg_train),n)
    mae_train.append(mae_reg_train)
    mae_reg_test=round(mean_absolute_error(y_test,y_reg_test),n)
    mae_test.append(mae_reg_test)
    mdae_reg_train=round(median_absolute_error(y_train,y_reg_train),n)
    mdae_train.append(mdae_reg_train)
    mdae_reg_test=round(median_absolute_error(y_test,y_reg_test),n)
    mdae_test.append(mdae_reg_test)    
    return [y_reg_train,y_reg_test,r2_reg_train,r2_reg_test,
            ev_reg_train,ev_reg_test,
            mse_reg_train,mse_reg_test,mae_reg_train,mae_reg_test,
            mdae_reg_train,mdae_reg_test]
def get_regressor_results():
    return pd.DataFrame({'regressor':regressor_list,
                         'regressor_name':regressor_names,
                         'dataset':reg_datasets,
                         'r2_train':r2_train,'r2_test':r2_test,
                         'ev_train':ev_train,'ev_test':ev_test,
                         'mse_train':mse_train,'mse_test':mse_test,
                         'mae_train':mae_train,'mae_test':mae_test,
                         'mdae_train':mdae_train,'mdae_test':mdae_test})

(x_train,y_train),(x_test,y_test)=boston_housing.load_data()
regressor_list,regressor_names,reg_datasets=[],[],[]
r2_train,r2_test,ev_train, ev_test,mse_train,mse_test,mae_train,\
mae_test,mdae_train,mdae_test=[],[],[],[],[],[],[],[],[],[]
df_list=['regressor_name','r2_train','r2_test','ev_train','ev_test',
         'mse_train','mse_test','mae_train','mae_test',
         'mdae_train','mdae_test']
reg=[linear_model.LinearRegression(),
     linear_model.Ridge(max_iter=800),
     linear_model.RidgeCV(),
     linear_model.Lasso(max_iter=800),
     linear_model.LassoLarsCV(max_iter=800),
     linear_model.RANSACRegressor(),
     linear_model.BayesianRidge(),
     linear_model.ARDRegression(),
     linear_model.HuberRegressor(max_iter=800),
     linear_model.TheilSenRegressor(max_iter=800),
     PLSRegression(),DecisionTreeRegressor(),ExtraTreeRegressor(),
     BaggingRegressor(),AdaBoostRegressor(),
     GradientBoostingRegressor(),RandomForestRegressor(),
     linear_model.PassiveAggressiveRegressor(max_iter=800,tol=.001),
     linear_model.ElasticNet(max_iter=800),
     linear_model.SGDRegressor(max_iter=800,tol=.001),
     svm.SVR(),KNeighborsRegressor(),
     RadiusNeighborsRegressor(radius=1.5),GaussianProcessRegressor()]

listreg=['LinearRegression','Ridge','RidgeCV',
         'Lasso','LassoLarsCV','RANSACRegressor',
         'BayesianRidge','ARDRegression','HuberRegressor',
         'TheilSenRegressor','PLSRegression','DecisionTreeRegressor',
         'ExtraTreeRegressor','BaggingRegressor','AdaBoostRegressor',
         'GradientBoostingRegressor','RandomForestRegressor']
yreg=[]
for i in range(len(listreg)):
    yreg.append(regressor_fit_score(
        reg[i],listreg[i],'Boston',x_train,x_test,y_train,y_test)[:2])
[[y_train101,y_test101],[y_train102,y_test102],[y_train103,y_test103],
 [y_train104,y_test104],[y_train105,y_test105],[y_train106,y_test106],
 [y_train107,y_test107],[y_train108,y_test108],[y_train109,y_test109],
 [y_train110,y_test110],[y_train111,y_test111],[y_train112,y_test112],
 [y_train113,y_test113],[y_train114,y_test114],[y_train115,y_test115],
 [y_train116,y_test116],[y_train117,y_test117]]=yreg

df_regressor_results=get_regressor_results()
df_regressor_results.to_csv('regressor_results.csv')
df_regressor_results[df_list]\
.sort_values('r2_test',ascending=False).style.set_precision(6)

pl.figure(figsize=(12,6)); n=30; x=range(n)
pl.scatter(x,y_test[:n],marker='*',s=100,
           color='black',label='Real data')
pl.plot(x,y_test116[:n],lw=2,label='Gradient Boosting')
pl.plot(x,y_test117[:n],lw=2,label='Random Forest')
pl.plot(x,y_test114[:n],lw=2,label='Bagging')
pl.plot(x,y_test115[:n],lw=2,label='Ada Boost')
pl.plot(x,y_test113[:n],lw=2,label='ExtraTree')
pl.xlabel('Observations'); pl.ylabel('Targets')
pl.title('Regressors. Test Results. Boston')
pl.legend(loc=2,fontsize=10); pl.show()

"""## ‚úíÔ∏è  Step 5. Neural Networks R 


"""

import warnings; warnings.filterwarnings('ignore')
from IPython import display
import rpy2.robjects as ro,pylab as pl,pandas as pd
from rpy2.robjects.packages import importr
grdevices=importr('grDevices')
grdevices.png(file='Rpy2.png',width=800,height=400)
ro.r('library("MASS"); library("nnet"); '+\
     'data(Boston); n<-dim(Boston)[1]')
ro.r('model<-nnet(as.matrix(Boston[1:430,-14]),'+\
     'as.matrix(Boston[1:430,14]),'+\
     'size=52,trace=FALSE,maxit=10^3,linout=TRUE,decay=.1^5); '+\
     'predictions<-predict(model,'+\
     'as.matrix(Boston[431:n,-14]),type="raw")')
ro.r('plot(as.matrix(Boston[431:n,14]),col="black",type="o",'+\
     'xlab="",ylab="",yaxt="n"); par(new=TRUE); '+\
     'plot(predictions,col="#348abd",type="o",'+\
     'cex=1.3,ylab="Targets & Predictions"); grid();')   
grdevices.dev_off()
display.Image('Rpy2.png')

"""```
%%r
library('MASS'); library('nnet')
data(Boston); n<-dim(Boston)[1];
svg(filename='Rplots.svg',width=10,height=6,pointsize=12,onefile=TRUE,
    family='times',bg='white',antialias=c('default','none','gray','subpixel'))
model<-nnet(as.matrix(Boston[1:430,-14]),as.matrix(Boston[1:430,14]),
            size=65,trace=FALSE,maxit=10^3,linout=TRUE,decay=.1^5)
predictions<-predict(model,as.matrix(Boston[431:n,-14]),type='raw')
plot(as.matrix(Boston[431:n,14]),col='black',type='o',
     xlab='',ylab='',yaxt='n'); par(new=TRUE)
plot(predictions,col='#348abd',type='o',
     cex=1.3,ylab='Targets & Predictions')
grid(); dev.off()
```

## ‚úíÔ∏è  Addition. Combine R & Python
"""

import warnings as wr; wr.filterwarnings('ignore')
from sklearn import datasets
from IPython import display
import rpy2.robjects as ro,pylab as pl,pandas as pd
from rpy2.robjects.packages import importr
from rpy2.robjects.conversion import localconverter as lc
from rpy2.robjects import r,pandas2ri
from rpy2.robjects.pandas2ri import py2rpy,rpy2py
pandas2ri.activate()

#the 1st method
base=importr('base'); grdevices=importr('grDevices')
bd=datasets.load_boston()
bd=pd.DataFrame(bd.data,columns=bd.feature_names)
bd[['PTRATIO','LSTAT','RAD']].plot(kind='line',figsize=(10,4))
pl.tight_layout(); pl.show()
with lc(ro.default_converter+pandas2ri.converter):
    print(base.summary(bd))

#the 2nd method
utils=importr('utils'); grdevices=importr('grDevices')
rbd=py2rpy(bd)
utils.write_table(rbd,file='rbd.csv',sep=',',row_names=False)
print(base.summary(rbd))
grdevices.png(file='Rpy2.png',width=800,height=400)
r('rbd<-read.csv("rbd.csv")')
r('matplot(c(1:506),rbd[,c(9,11,13)],type="l")')
r('grid()')
grdevices.dev_off(); display.Image('Rpy2.png')