# -*- coding: utf-8 -*-
"""sb_housing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13YVEPwrMQC2TO2GGPvV95Bysh4dhd8we

# Code Modules & Helpful Functions
"""

import warnings; warnings.filterwarnings("ignore")
from sklearn.exceptions import DataConversionWarning
warnings.filterwarnings("ignore",category=DataConversionWarning)
import numpy,pandas,pylab,seaborn,sympy,keras
pylab.style.use('seaborn-whitegrid')
from sklearn.feature_selection import SelectKBest,chi2,RFE
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.metrics import mean_squared_error,median_absolute_error,r2_score
from sklearn.metrics import mean_absolute_error,explained_variance_score
from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor
from sklearn.ensemble import BaggingRegressor,AdaBoostRegressor,ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression,SGDRegressor,RANSACRegressor
from sklearn.linear_model import Ridge,RidgeCV,BayesianRidge
from sklearn.linear_model import HuberRegressor,TheilSenRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler
from keras.models import Sequential,Model
from keras.layers import Conv1D,Conv2D,MaxPooling1D,MaxPooling2D
from keras.layers import GlobalAveragePooling2D,GlobalMaxPooling2D
from keras.layers.advanced_activations import PReLU,LeakyReLU
from keras.layers import Input,Dense,LSTM,Activation,Flatten,Dropout

def display_cat(df,feature):
    print ('\n'+'<_>'*30); print ('\n'+feature+': '+str(len(set(df[feature]))))
    sympy.pprint(set(df[feature]))
def robust_scaler(Xtrain,ytrain,Xtest,ytest):
    scale_X=RobustScaler(); scale_y=RobustScaler()
    Xtrain=scale_X.fit_transform(Xtrain); Xtest=scale_X.transform(Xtest)
    ytrain=scale_y.fit_transform(ytrain.reshape(-1,1))
    ytest=scale_y.transform(ytest.reshape(-1,1))
    return Xtrain,ytrain,Xtest,ytest
def regression(regressor,X_train,X_test,y_train):
    regressor.fit(X_train,y_train)
    return regressor.predict(X_train),regressor.predict(X_test)
def scores(title,y_train,y_test,y_train_reg,y_test_reg):
    separator='-_-'*10; sympy.pprint(title); print('<_>'*38)
    print("EV score. Train: "+str(explained_variance_score(y_train,y_train_reg)))
    print("EV score. Test: "+str(explained_variance_score(y_test,y_test_reg)))
    print(separator)
    print("R2 score. Train: "+str(r2_score(y_train,y_train_reg)))
    print("R2 score. Test: "+str(r2_score(y_test,y_test_reg)))
    print(separator)
    print("MSE score. Train: "+str(mean_squared_error(y_train,y_train_reg)))
    print("MSE score. Test: "+str(mean_squared_error(y_test,y_test_reg)))
    print(separator)
    print("MAE score. Train: "+str(mean_absolute_error(y_train,y_train_reg)))
    print("MAE score. Test: "+str(mean_absolute_error(y_test,y_test_reg)))
    print(separator)
    print("MdAE score. Train: "+str(median_absolute_error(y_train,y_train_reg)))
    print("MdAE score. Test: "+str(median_absolute_error(y_test,y_test_reg))) 
def display_importance(regressor,X,y,title,n):
    importances=regressor.fit(X,y).feature_importances_
    indices=numpy.argsort(importances)[::-int(1)][:int(n)]
    pylab.figure(figsize=(12,4))
    pylab.xlabel("Feature Index"); pylab.ylabel("Feature Importance")
    pylab.bar(range(n),importances[indices],
              color="forestgreen",align="center",alpha=.5)
    pylab.xticks(range(n),indices); pylab.title(title); pylab.show()
def history_plot(fit_history):
    keys=list(fit_history.history.keys())[0:4]
    pylab.figure(figsize=(12,8)); pylab.subplot(211)
    pylab.plot(fit_history.history[keys[0]],color='slategray',label='train')
    pylab.plot(fit_history.history[keys[2]],color='#228B22',label='test')
    pylab.xlabel("Epochs"); pylab.ylabel("Loss")
    pylab.legend(); pylab.title('Loss Function')    
    pylab.subplot(212)
    pylab.plot(fit_history.history[keys[1]],
               color='slategray',label='train')
    pylab.plot(fit_history.history[keys[3]],
               color='#228B22',label='test')
    pylab.xlabel("Epochs"); pylab.ylabel("MAE"); pylab.legend()
    pylab.title('Mean Absolute Error'); pylab.show()

"""# Loading and Displaying the Data"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <div id="data">
#   <iframe src="https://olgabelitskaya.github.io/data_dictionary.txt" 
#   height="200" width="98%"></iframe>
# </div>

path='https://raw.githubusercontent.com/OlgaBelitskaya/'+\
     'machine_learning_engineer_nd009/master/Machine_Learning_Engineer_ND_P6/'
macro=pandas.read_csv(path+'macro.csv')
train=pandas.read_csv(path+'train.csv'); test=pandas.read_csv(path+'test.csv')
n1,n2,n3,n4=int(100),int(107),int(1),int(15)
macro[n1:n2].T[n3:n4]

train[n1:n2].T[n3:n4]

"""# Selection of Features"""

X_list_num=['timestamp','full_sq','floor','max_floor','num_room','area_m',
            'kremlin_km','big_road2_km','big_road1_km','workplaces_km',
            'stadium_km','swim_pool_km','fitness_km','detention_facility_km',
            'cemetery_km','radiation_km','oil_chemistry_km',
            'theater_km','exhibition_km','museum_km','park_km',
            'public_healthcare_km','metro_min_walk','metro_km_avto', 
            'bus_terminal_avto_km','public_transport_station_min_walk',
            'railroad_station_walk_min','railroad_station_avto_km',
            'kindergarten_km','school_km','preschool_km','university_km',
            'additional_education_km','shopping_centers_km',
            'big_market_km','ekder_all','work_all','young_all']
X_list_cat=['sub_area','ID_metro','office_raion','sport_objects_raion',
            'raion_popul','healthcare_centers_raion',
            'school_education_centers_raion',
            'preschool_education_centers_raion']
target_train=train['price_doc']

f,(ax1,ax2)=pylab.subplots(ncols=2,figsize=(11,5))
seaborn.distplot(target_train,bins=200,color='#228B22',ax=ax1)
ax1.set_xlabel("Prices"); ax1.set_ylabel("Distribution")
seaborn.distplot(numpy.log(target_train),bins=200,color='#228B22',ax=ax2)
ax2.set_xlabel("Logarithm of the variable 'Prices'")
ax2.set_ylabel("Distribution")
pylab.suptitle('Sberbank Russian Housing Data'); pylab.show()

print ("Sberbank Russian Housing Dataset Statistics: \n")
print ("Number of houses = "+str(len(target_train)))
print ("Number of features = "+str(len(list(train[X_list_num+X_list_cat].keys()))))
print ("Minimum house price = "+str(numpy.min(target_train)))
print ("Maximum house price = "+str(numpy.max(target_train)))
print ("Mean house price = %.4f"%numpy.mean(target_train))
print ("Median house price = %.4f" %numpy.median(target_train))
print ("Standard deviation of house prices = %.4f"%numpy.std(target_train))

"""# Fill in Missing Values"""

pandas.DataFrame(train[X_list_num].isnull()\
.sum()[train[X_list_num].isnull().sum()!=0],columns=['nan'])

pandas.DataFrame(test[X_list_num].isnull()\
.sum()[test[X_list_num].isnull().sum()!=0],columns=['nan'])

df_train=pandas.DataFrame(train,columns=X_list_num)
df_train_cat=pandas.DataFrame(train,columns=X_list_num+X_list_cat)
df_test=pandas.DataFrame(test,columns=X_list_num)
df_test_cat=pandas.DataFrame(test,columns=X_list_num+X_list_cat)
df_train['prices']=target_train; df_train_cat['prices']=target_train
df_train=df_train.dropna(subset=['num_room'])
df_train_cat=df_train_cat.dropna(subset=['num_room'])
for el in ['metro_min_walk','railroad_station_walk_min','floor']:
    for df in [df_train,df_train_cat,df_test,df_test_cat]:
        df[el]=df[el].interpolate(method='linear')
len(df_train),len(df_test)

#pandas.DataFrame(df_test[X_list_num].isnull().sum(),columns=['nan'])

"""# Categorical and Macro Features
## Add One Macro Feature
"""

usdrub_pairs=dict(zip(list(macro['timestamp']),list(macro['usdrub'])))
# salary_pairs=dict(zip(list(macro['timestamp']),list(macro['salary'])))
for df in [df_train,df_train_cat,df_test,df_test_cat]:
    df['timestamp'].replace(usdrub_pairs,inplace=True)
    df.rename(columns={'timestamp':'usdrub'},inplace=True)

"""## Explore numbers of categories and values for categorical features"""

for feature in X_list_cat:
    display_cat(df_train_cat,feature)

for feature in X_list_cat:
    display_cat(df_test_cat,feature)

# Find the missing category in the testing set
for feature in X_list_cat:
    for element in list(set(df_test_cat[feature])):
        if element not in list(set(df_train_cat[feature])): 
            print (feature,element)

# Replace categorical values of 'ID_metro' by discrete numbers
ID_metro_cat=pandas.factorize(df_train_cat['ID_metro'])
ID_metro_pairs=dict(zip(list(ID_metro_cat[1]),list(set(ID_metro_cat[0]))))
ID_metro_pairs[224]=219
df_train_cat['ID_metro']=ID_metro_cat[0]
df_test_cat['ID_metro'].replace(ID_metro_pairs,inplace=True)

# Replace values of other categorical features by discrete numbers
for feature in X_list_cat:
    if feature!='ID_metro':
        feature_cat=pandas.factorize(df_train_cat[feature])
        feature_pairs=dict(zip(list(feature_cat[1]),list(set(feature_cat[0]))))
        df_train_cat[feature]=feature_cat[0]
        df_test_cat[feature].replace(feature_pairs,inplace=True)

for feature in X_list_cat:
    display_cat(df_train_cat,feature)

for feature in X_list_cat:
    display_cat(df_test_cat,feature)

# Apply one hot encoding for the training set
encode=OneHotEncoder(sparse=False); df_train_cat1=df_train_cat.copy()
for column in X_list_cat:
    encode.fit(df_train_cat[[column]])
    transform=encode.transform(df_train_cat[[column]])    
    transform=pandas.DataFrame(transform,columns=[(column+"_"+str(i)) 
    for i in df_train_cat[column].value_counts().index])
    transform=transform.set_index(df_train_cat.index.values)    
    df_train_cat1=pandas.concat([df_train_cat1,transform],axis=int(1))
    df_train_cat1=df_train_cat1.drop(column,1)

# Apply one hot encoding for the testing set
encode=OneHotEncoder(sparse=False); df_test_cat1=df_test_cat.copy()
for column in X_list_cat:
    encode.fit(df_test_cat[[column]])
    transform=encode.transform(df_test_cat[[column]])    
    transform=pandas.DataFrame(transform,columns=[(column+"_"+str(i)) 
    for i in df_test_cat[column].value_counts().index])
    transform=transform.set_index(df_test_cat.index.values)    
    df_test_cat1=pandas.concat([df_test_cat1,transform],axis=int(1))
    df_test_cat1=df_test_cat1.drop(column,1)

# Check encoding
n1,n2,n3=int(625),int(638),int(3)
df_train_cat1.iloc[:,n1:n2][:n3].values

df_train_cat['preschool_education_centers_raion'][:n3]

"""## Add Missing Columns with Zero Values"""

print('Shape of the train data frame:'+str(df_train_cat1.shape))
print('Shape of the test data frame:'+str(df_test_cat1.shape))
print("Features in the train data, but not in the test data:")
for element in list(df_train_cat1):
    if element not in list(df_test_cat1):
        print(element)
print("Features in the test data, but not in the train data:")
for element in list(df_test_cat1):
    if element not in list(df_train_cat1):
        print(element)

for column in ['sub_area_136','ID_metro_188','ID_metro_205',
               'ID_metro_216','ID_metro_214',
               'ID_metro_183','ID_metro_179','ID_metro_153',
               'ID_metro_217','raion_popul_136']:
    df_test_cat1[column]=0    
df_train_cat1['ID_metro_219']=0
print('Columns with zero values were added.\n')
print('Shape of the train data frame:'+str(df_train_cat1.shape))
print('Shape of the test data frame:'+str(df_test_cat1.shape))

"""# Display Correlation"""

corr_with_prices=df_train.corr(method='pearson').iloc[-int(1)][:-int(1)]
pandas.DataFrame(corr_with_prices[abs(corr_with_prices).argsort()[::-int(1)]])

features_list=corr_with_prices[abs(corr_with_prices)\
.argsort()[::-int(1)]][:int(10)].index.values.tolist()
print('The most correlated with prices:\n'); features_list

pylab.figure(figsize=(12,12)); cols=df_train.corr().columns.values
seaborn.heatmap(df_train.corr(),cmap=pylab.cm.Greens,
                xticklabels=cols,yticklabels=cols)
pylab.title("Correlation Matrix",fontsize=15)
pylab.xticks(fontsize=5); pylab.yticks(fontsize=6); pylab.show()

"""# Scale, Shuffle and Split the Data"""

target_train,features_train=\
df_train['prices'].values,df_train.drop('prices',int(1)).values
features_train_cat,features_train_cat_enc=\
df_train_cat.drop('prices',int(1)).values,\
df_train_cat1.drop('prices',int(1)).values
features_test,features_test_cat,features_test_cat_enc=\
df_test.values,df_test_cat.values,df_test_cat1.values

sympy.pprint('Numeric Features')
X_train,X_test,y_train,y_test=\
train_test_split(features_train,target_train,test_size=.2,random_state=1)
X_train.shape,X_test.shape,y_train.shape,y_test.shape

sympy.pprint('Numeric and Categorical Features')
X_train_cat,X_test_cat,y_train_cat,y_test_cat=\
train_test_split(features_train_cat,target_train,test_size=.2,random_state=1)
X_train_cat.shape,X_test_cat.shape,y_train_cat.shape,y_test_cat.shape

sympy.pprint('Numeric and Encoded Categorical Features')
X_train_cat_enc,X_test_cat_enc,y_train_cat_enc,y_test_cat_enc=\
train_test_split(features_train_cat_enc,target_train,test_size=.2,random_state=1)
X_train_cat_enc.shape,X_test_cat_enc.shape,\
y_train_cat_enc.shape,y_test_cat_enc.shape

#for [Xtrain,ytrain,Xtest,ytest] in 
#[[X_train,y_train,X_test,y_test],
# [X_train_cat,y_train_cat,X_test_cat,y_test_cat],
# [X_train_cat_enc,y_train_cat_enc,X_test_cat_enc,y_test_cat_enc]]:
#    Xtrain,ytrain,Xtest,ytest=robust_scaler(Xtrain,ytrain,Xtest,ytest)

"""# Benchmark Models
## Ensemble Regressors. Scikit-Learn
Tuning Parameters
"""

sympy.pprint('Numeric Features')
param_gbr={'max_depth':[3,4,5],'n_estimators':range(38,381,38)}
#gridsearch_gbr=GridSearchCV(GradientBoostingRegressor(),param_gbr).fit(X_train,y_train)
#gridsearch_gbr.best_params_

sympy.pprint('Numeric and Categorical Features')
param_gbr_cat={'max_depth':[3,4,5],'n_estimators':range(46,461,46)}
#gridsearch_gbr_cat=\
#GridSearchCV(GradientBoostingRegressor(),param_gbr_cat)\
#.fit(X_train_cat,y_train_cat)
#gridsearch_gbr_cat.best_params_

sympy.pprint('Numeric Features and Encoded Categorical Features')
param_gbr_cat_enc={'max_depth':[3,4,5],'n_estimators':[160,319,638]}
#gridsearch_gbr_cat_enc=\
#GridSearchCV(GradientBoostingRegressor(),param_gbr_cat_enc)\
#.fit(X_train_cat_enc,y_train_cat_enc)
#gridsearch_gbr_cat_enc.best_params_

"""Fit Regressors"""

y_train_gbr,y_test_gbr=\
regression(GradientBoostingRegressor(max_depth=4,n_estimators=342),
           X_train,X_test,y_train)
scores('GradientBoostingRegressor. Numeric Features',
       y_train,y_test,y_train_gbr,y_test_gbr)

GradientBoostingRegressor(max_depth=4,n_estimators=342)\
.get_params(deep=True)

ti="Importance of Numeric Features. Gradient Boosting Regressor"
display_importance(GradientBoostingRegressor(max_depth=4,n_estimators=324),
                   X_train,y_train,ti,38)

y_train_cat_gbr,y_test_cat_gbr=\
regression(GradientBoostingRegressor(max_depth=4,n_estimators=308),
           X_train_cat,X_test_cat,y_train_cat)
ti='GradientBoostingRegressor. Numeric and Categorical Features'
scores(ti,y_train_cat,y_test_cat,y_train_cat_gbr,y_test_cat_gbr)

ti="Importance of Numeric and Categorical Features. Gradient Boosting Regressor"
display_importance(GradientBoostingRegressor(max_depth=4,n_estimators=308),
                   X_train_cat,y_train_cat,ti,46)

y_train_cat_enc_gbr,y_test_cat_enc_gbr=\
regression(GradientBoostingRegressor(max_depth=3,n_estimators=159),
           X_train_cat_enc,X_test_cat_enc,y_train_cat_enc)
scores('GradientBoostingRegressor. Numeric and Encoded Categorical Features',
       y_train_cat_enc,y_test_cat_enc,y_train_cat_enc_gbr,y_test_cat_enc_gbr)

ti="Importance of Numeric and Categorical Encoded Features. "+\
   "Gradient Boosting Regressor"
display_importance(GradientBoostingRegressor(max_depth=3,n_estimators=159),
                   X_train_cat_enc,y_train_cat_enc,ti,30)

"""Fit Regressors with Dimensionality Reduction"""

pca=PCA(n_components=11).fit(X_train)
X_train_pca=pca.transform(X_train); X_test_pca=pca.transform(X_test)
y_train_gbr_pca,y_test_gbr_pca=\
regression(GradientBoostingRegressor(max_depth=4,n_estimators=38*8),
           X_train_pca,X_test_pca,y_train)
scores('GradientBoostingRegressor. Numeric Features (PCA)',
       y_train,y_test,y_train_gbr_pca,y_test_gbr_pca)

ti="Importance of Numeric Features (PCA). Gradient Boosting Regressor"
display_importance(GradientBoostingRegressor(max_depth=4,n_estimators=38*8),
                   X_train_pca,y_train,ti,11)

pca_cat=PCA(n_components=13).fit(X_train_cat)
X_train_cat_pca=pca_cat.transform(X_train_cat)
X_test_cat_pca=pca_cat.transform(X_test_cat)
y_train_cat_gbr_pca,y_test_cat_gbr_pca=\
regression(GradientBoostingRegressor(max_depth=4,n_estimators=32*8),
           X_train_cat_pca,X_test_cat_pca,y_train_cat)
scores('GradientBoostingRegressor. Numeric and Categorical Features (PCA)',
       y_train_cat,y_test_cat,y_train_cat_gbr_pca,y_test_cat_gbr_pca)

ti="Importance of Numeric and Categorical Features (PCA). Gradient Boosting Regressor"
display_importance(GradientBoostingRegressor(max_depth=4,n_estimators=32*8),
                   X_train_cat_pca,y_train_cat,ti,13)

pca_cat_enc=PCA(n_components=18).fit(X_train_cat_enc)
X_train_cat_enc_pca=pca_cat_enc.transform(X_train_cat_enc)
X_test_cat_enc_pca=pca_cat_enc.transform(X_test_cat_enc)
y_train_cat_enc_gbr_pca,y_test_cat_enc_gbr_pca=\
regression(GradientBoostingRegressor(max_depth=4,n_estimators=32*8),
           X_train_cat_enc_pca,X_test_cat_enc_pca,y_train_cat_enc)
scores('GradientBoostingRegressor. Numeric and Categorical Features (PCA)',
       y_train_cat_enc,y_test_cat_enc,
       y_train_cat_enc_gbr_pca,y_test_cat_enc_gbr_pca)

ti="Importance of Numeric and Categorical Encoded Features. Gradient Boosting Regressor(PCA)"
display_importance(GradientBoostingRegressor(max_depth=4,n_estimators=32*8),
                   X_train_cat_enc_pca,y_train_cat_enc,ti,18)

"""## MLP Regressors. Scikit-Learn
Fit Regressors
"""

mlpr=MLPRegressor(hidden_layer_sizes=(32*8,),max_iter=500,
                  solver='adam',batch_size=12,
                  learning_rate='adaptive',verbose='True')
mlpr.fit(X_train,y_train)

y_train_mlpr=mlpr.predict(X_train)
y_test_mlpr=mlpr.predict(X_test)
scores('MLP Regressor. Numeric Features',
       y_train,y_test,y_train_mlpr,y_test_mlpr)

mlpr_cat=MLPRegressor(hidden_layer_sizes=(32*8,),max_iter=500,
                      solver='adam',batch_size=12,
                      learning_rate='adaptive',verbose='True')
mlpr_cat.fit(X_train_cat,y_train_cat)

y_train_cat_mlpr=mlpr_cat.predict(X_train_cat)
y_test_cat_mlpr=mlpr_cat.predict(X_test_cat)
scores('MLP Regressor. Numeric and Categorical Features',
       y_train_cat,y_test_cat,y_train_cat_mlpr,y_test_cat_mlpr)

mlpr_cat_enc=MLPRegressor(hidden_layer_sizes=(32*8,),max_iter=500,
                          solver='adam',batch_size=12,
                          learning_rate='adaptive',verbose='True')
mlpr_cat_enc.fit(X_train_cat_enc,y_train_cat_enc)

y_train_cat_enc_mlpr=mlpr_cat_enc.predict(X_train_cat_enc)
y_test_cat_enc_mlpr=mlpr_cat_enc.predict(X_test_cat_enc)
scores('MLP Regressor. Numeric and Categorical Encoded Features',
       y_train_cat_enc,y_test_cat_enc,y_train_cat_enc_mlpr,y_test_cat_enc_mlpr)

"""Fit Regressors with Dimensionality Reduction"""

mlpr_pca=MLPRegressor(hidden_layer_sizes=(32*8,),
                      max_iter=500,solver='adam',batch_size=14,
                      learning_rate='adaptive',verbose='True')
mlpr_pca.fit(X_train_pca,y_train)

y_train_mlpr_pca=mlpr_pca.predict(X_train_pca); y_test_mlpr_pca=mlpr_pca.predict(X_test_pca)
scores('MLP Regressor. Numeric Features (PCA)',y_train,y_test,y_train_mlpr_pca,y_test_mlpr_pca)

mlpr_cat_pca=MLPRegressor(hidden_layer_sizes=(32*8,),max_iter=100,solver='adam',
                          batch_size=12,learning_rate='adaptive',verbose='True')
mlpr_cat_pca.fit(X_train_cat_pca,y_train_cat)

y_train_cat_mlpr_pca=mlpr_cat_pca.predict(X_train_cat_pca)
y_test_cat_mlpr_pca=mlpr_cat_pca.predict(X_test_cat_pca)
scores('MLP Regressor. Numeric and Categorical Features (PCA)',
       y_train_cat,y_test_cat,y_train_cat_mlpr_pca,y_test_cat_mlpr_pca)

mlpr_cat_enc_pca=MLPRegressor(hidden_layer_sizes=(32*8,),max_iter=100,solver='adam',
                              batch_size=12,learning_rate='adaptive',verbose='True')
mlpr_cat_enc_pca.fit(X_train_cat_enc_pca,y_train_cat_enc)

y_train_cat_enc_mlpr_pca=mlpr_cat_enc_pca.predict(X_train_cat_enc_pca)
y_test_cat_enc_mlpr_pca=mlpr_cat_enc_pca.predict(X_test_cat_enc_pca)
scores('MLP Regressor. Numeric and Encoded Categorical Features (PCA)',
       y_train_cat,y_test_cat,y_train_cat_mlpr_pca,y_test_cat_mlpr_pca)

"""## Neural Networks. Keras
MLP
"""

def mlp_model():
    model=Sequential()
    model.add(Dense(32*32,activation='relu',input_dim=38))
    model.add(Dense(32*4,activation='relu'))
    model.add(Dense(32,activation='relu'))
    model.add(Dense(1,kernel_initializer='normal'))    
    model.compile(loss='mse',optimizer='nadam',metrics=['mae'])
    return model
mlp_model=mlp_model()
checkpointer=keras.callbacks.ModelCheckpoint(filepath='weights.best.model.hdf5',
                                             save_best_only=True)
lr_reduction=keras.callbacks.ReduceLROnPlateau(monitor='val_loss',
                                               verbose=2,patience=5,factor=0.95)
mlp_history=mlp_model.fit(X_train,y_train,validation_data=(X_test,y_test),
                          nb_epoch=600,batch_size=12,verbose=2,
                          callbacks=[checkpointer,lr_reduction])

history_plot(mlp_history)

mlp_model.load_weights('weights.best.model.hdf5')
y_train_mlp=mlp_model.predict(X_train); y_test_mlp=mlp_model.predict(X_test)
scores('MLP Initial Model. Numeric Features',
       y_train,y_test,y_train_mlp,y_test_mlp)

def mlp_cat_model():
    model = Sequential()    
    model.add(Dense(32*32,activation='relu',input_dim=46))
    model.add(Dense(32*4,activation='relu'))   
    model.add(Dense(1,kernel_initializer='normal'))    
    model.compile(loss='mse',optimizer='nadam',metrics=['mae'])
    return model
mlp_cat_model=mlp_cat_model()
checkpointer=keras.callbacks.ModelCheckpoint(filepath='weights.best.model.hdf5',
                                             save_best_only=True)
lr_reduction=keras.callbacks.ReduceLROnPlateau(monitor='val_loss',
                                               verbose=2,patience=5,factor=0.95)
mlp_cat_history=mlp_cat_model.fit(X_train_cat,y_train_cat,
                                  validation_data=(X_test_cat,y_test_cat),
                                  nb_epoch=600,batch_size=12,verbose=2,
                                  callbacks=[checkpointer,lr_reduction])

history_plot(mlp_cat_history)

mlp_cat_model.load_weights('weights.best.model.hdf5')
y_train_cat_mlp=mlp_cat_model.predict(X_train_cat)
y_test_cat_mlp=mlp_cat_model.predict(X_test_cat)
scores('MLP Initial Model. Numeric and Categorical Features',
       y_train_cat,y_test_cat,y_train_cat_mlp,y_test_cat_mlp)

def mlp_cat_enc_model():
    model=Sequential()
    model.add(Dense(32*32,activation='relu',input_dim=638))    
    model.add(Dense(1,kernel_initializer='normal'))    
    model.compile(loss='mse',optimizer='rmsprop',metrics=['mae'])
    return model
mlp_cat_enc_model=mlp_cat_enc_model()
checkpointer=keras.callbacks.ModelCheckpoint(filepath='weights.best.model.hdf5',
                                             save_best_only=True)
lr_reduction=keras.callbacks.ReduceLROnPlateau(monitor='val_loss',
                                               verbose=2,patience=5,factor=0.9)
mlp_cat_enc_history=mlp_cat_enc_model.fit(X_train_cat_enc,y_train_cat_enc,
                                          validation_data=(X_test_cat_enc,y_test_cat_enc),
                                          nb_epoch=1000,batch_size=16,verbose=2,
                                          callbacks=[checkpointer,lr_reduction])

history_plot(mlp_cat_enc_history)

mlp_cat_enc_model.load_weights('weights.best.model.hdf5')
y_train_cat_enc_mlp=mlp_cat_enc_model.predict(X_train_cat_enc)
y_test_cat_enc_mlp=mlp_cat_enc_model.predict(X_test_cat_enc)
scores('MLP Initial Model. Numeric and Categorical Encoded Features',
       y_train_cat_enc,y_test_cat_enc,y_train_cat_enc_mlp,y_test_cat_enc_mlp)

"""# Display Predictions"""

n=int(50); pylab.figure(figsize=(12,5))
pylab.plot(y_test[1:n],color='black',label='Real Data')
pylab.plot(y_test_gbr[1:n],label='Gradient Boosting')
pylab.plot(y_test_mlp[1:n],label='MLP Regressor')
pylab.legend(); pylab.title("Numeric Features; "+\
                            "Regressor Predictions vs Real Data"); pylab.show()

n=int(50); pylab.figure(figsize=(12,5)); pylab.plot(y_test_cat[1:n],color='black',label='Real Data')
pylab.plot(y_test_cat_gbr[1:n],label='Gradient Boosting'); pylab.plot(y_test_cat_mlp[1:n],label='MLP Regressor')
pylab.legend(); pylab.title("Numeric and Categorical Features; Regressor Predictions vs Real Data"); pylab.show()

n=int(50); pylab.figure(figsize=(12,5)); pylab.plot(y_test_cat_enc[1:n],color='black',label='Real Data')
pylab.plot(y_test_cat_enc_gbr[1:n],label='Gradient Boosting'); pylab.plot(y_test_cat_enc_mlp[1:n],label='MLP Regressor')
pylab.legend(); pylab.title("Numeric and Categorical Encoded Features; Regressor Predictions vs Real Data"); pylab.show()