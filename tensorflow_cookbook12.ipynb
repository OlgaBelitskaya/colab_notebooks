{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_cookbook12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSRex4LmRtSp",
        "cellView": "form"
      },
      "source": [
        "#@title Code Lines with Header Styling\n",
        "from IPython.display import display,HTML\n",
        "from IPython.core.magic import register_line_magic\n",
        "@register_line_magic\n",
        "def cmap_header(params):\n",
        "    params=params.split('|'); string=params[0]\n",
        "    if len(params)==1: \n",
        "        font_size='30'; font_family='Smokum'; cmap='Sinebow'\n",
        "    elif  len(params)==2: \n",
        "        font_size=params[1]; font_family='Smokum'; cmap='Sinebow'\n",
        "    elif  len(params)==3: \n",
        "        font_size=params[1]; font_family=params[2]; cmap='Sinebow'\n",
        "    else: \n",
        "        font_size=params[1]; font_family=params[2]; cmap=params[3]\n",
        "    html_str=\"\"\"\n",
        "    <head><script src='https://d3js.org/d3.v6.min.js'></script></head>\n",
        "    <style>@import 'https://fonts.googleapis.com/css?family=\"\"\"+\\\n",
        "    font_family+\"\"\"&effect=3d'; #colorized {font-family:\"\"\"+font_family+\"\"\"; \n",
        "    color:white; padding-left:10px; font-size:\"\"\"+font_size+\"\"\"px;}</style>\n",
        "    <h1 id='colorized' class='font-effect-3d'>\"\"\"+string+\"\"\"</h1>\n",
        "    <script>\n",
        "    var tc=setInterval(function(){\n",
        "        var now=new Date().getTime();\n",
        "        var iddoc=document.getElementById('colorized');\n",
        "        iddoc.style.color=d3.interpolate\"\"\"+cmap+\"\"\"(now%(60000)/60000);},1)\n",
        "    </script>\"\"\"\n",
        "    display(HTML(html_str))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "hrflL_XER9Xh",
        "outputId": "a48a6276-58da-4fc7-8763-32a554869e9e"
      },
      "source": [
        "%cmap_header Code Modules & Functions"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <head><script src='https://d3js.org/d3.v6.min.js'></script></head>\n",
              "    <style>@import 'https://fonts.googleapis.com/css?family=Smokum&effect=3d'; #colorized {font-family:Smokum; \n",
              "    color:white; padding-left:10px; font-size:30px;}</style>\n",
              "    <h1 id='colorized' class='font-effect-3d'>Code Modules & Functions</h1>\n",
              "    <script>\n",
              "    var tc=setInterval(function(){\n",
              "        var now=new Date().getTime();\n",
              "        var iddoc=document.getElementById('colorized');\n",
              "        iddoc.style.color=d3.interpolateSinebow(now%(60000)/60000);},1)\n",
              "    </script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDLRAwpVt4P-"
      },
      "source": [
        "!python -m pip install --upgrade pip --user --quiet \n",
        "!python -m pip install --upgrade \\\n",
        "neural_structured_learning --user --quiet "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPMAzTEOxYR"
      },
      "source": [
        "import os,h5py,urllib,pandas as pd,numpy as np\n",
        "import tensorflow as tf,pylab as pl,seaborn as sn\n",
        "import neural_structured_learning as nsl\n",
        "import tensorflow.keras.layers as tkl\n",
        "import tensorflow.keras.callbacks as tkc\n",
        "file_path='https://raw.githubusercontent.com/'+\\\n",
        "          'OlgaBelitskaya/data_kitchen/main/'\n",
        "file_name='Pictograms64.h5'\n",
        "img_size=64\n",
        "model_weights='/checkpoints'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsew-EBLSntR"
      },
      "source": [
        "def h5file2data(h5file,img_size,\n",
        "                resize=True,cmap='tab20',fig_size=7):\n",
        "    with h5py.File(h5file,'r') as f:\n",
        "        keys=list(f.keys()); print('file keys: '+', '.join(keys))\n",
        "        images=np.array(f[keys[int(0)]],dtype='float32')\n",
        "        labels=np.array(f[keys[int(1)]])\n",
        "        names=[[el.decode('utf-8') for el in f[keys[i]]]\n",
        "               for i in range(2,len(keys))]\n",
        "        f.close()\n",
        "    N=images.shape[0]; n=int(.1*N)\n",
        "    if resize:\n",
        "        images=tf.image.resize(images,[img_size,img_size]).numpy()\n",
        "    shuffle_ids=np.arange(N)\n",
        "    np.random.RandomState(12).shuffle(shuffle_ids)\n",
        "    images=images[shuffle_ids]\n",
        "    labels=np.array([labels[i][shuffle_ids] \n",
        "                     for i in range(labels.shape[0])])\n",
        "    x_test,x_valid,x_train=images[:n],images[n:2*n],images[2*n:]\n",
        "    y_test,y_valid,y_train=labels[:,:n],labels[:,n:2*n],labels[:,2*n:]\n",
        "    df=pd.DataFrame(\n",
        "        [[x_train.shape,x_valid.shape,x_test.shape],\n",
        "         [x_train.dtype,x_valid.dtype,x_test.dtype],\n",
        "         [y_train.shape,y_valid.shape,y_test.shape],\n",
        "         [y_train.dtype,y_valid.dtype,y_test.dtype]],\n",
        "        columns=['train','valid','test'],\n",
        "        index=['image shape','image type','label shape','label type'])\n",
        "    print('data outputs: '); display(df)\n",
        "    print('distribution of labels: ')\n",
        "    idx=['labels %d'%(i+1) for i in range(labels.shape[0])]\n",
        "    df=pd.DataFrame(labels,index=idx).T\n",
        "    for i in range(labels.shape[0]):\n",
        "        df['name %d'%(i+1)]=[names[i][l] for l in labels[i]]\n",
        "    fig=pl.figure(figsize=(1.5*fig_size,fig_size))    \n",
        "    for i in range(labels.shape[0]):\n",
        "        ax=fig.add_subplot(labels.shape[0],1,i+1)\n",
        "        sn.countplot(x='name %s'%(i+1),data=df,palette=cmap,alpha=.5,ax=ax)\n",
        "    pl.tight_layout(); pl.show()       \n",
        "    return [names,x_train,x_valid,x_test,y_train,y_valid,y_test]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIk7hlqbTzqw"
      },
      "source": [
        "def display_images(images,labels,names,n):\n",
        "    fig=pl.figure(figsize=(9,n/2))\n",
        "    randch=np.random.choice(images.shape[0],size=n,replace=False)\n",
        "    for i,idx in enumerate(randch):\n",
        "        ax=fig.add_subplot(int(n//5)+1,5,i+1,xticks=[],yticks=[])\n",
        "        ax.imshow(images[idx],cmap='bone')\n",
        "        label=[labels[:,idx]]\n",
        "        name=[names[i][labels[i][idx]] for i in range(labels.shape[0])]\n",
        "        ti='{} \\n {}'.format(str(label),str(name))\n",
        "        ax.set_title(ti,fontsize=8)\n",
        "    pl.tight_layout(); pl.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp8JWWzwwwkC"
      },
      "source": [
        "def cb(mw):\n",
        "    early_stopping=tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_sparse_categorical_accuracy',\n",
        "        patience=20,verbose=2,mode='max')\n",
        "    checkpointer=tf.keras.callbacks.ModelCheckpoint(\n",
        "        save_best_only=True,save_weights_only=True,\n",
        "        monitor='val_sparse_categorical_accuracy',\n",
        "        filepath=mw,verbose=2,mode='max')\n",
        "    lr_reduction=tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_sparse_categorical_accuracy',\n",
        "        verbose=2,patience=5,factor=.8,mode='max')\n",
        "    return [checkpointer,early_stopping,lr_reduction]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "E_P1XId4UEYK",
        "outputId": "7d0a0d1b-932e-4993-f3c7-1abc57d80fee"
      },
      "source": [
        "%cmap_header Data Loading"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <head><script src='https://d3js.org/d3.v6.min.js'></script></head>\n",
              "    <style>@import 'https://fonts.googleapis.com/css?family=Smokum&effect=3d'; #colorized {font-family:Smokum; \n",
              "    color:white; padding-left:10px; font-size:30px;}</style>\n",
              "    <h1 id='colorized' class='font-effect-3d'>Data Loading</h1>\n",
              "    <script>\n",
              "    var tc=setInterval(function(){\n",
              "        var now=new Date().getTime();\n",
              "        var iddoc=document.getElementById('colorized');\n",
              "        iddoc.style.color=d3.interpolateSinebow(now%(60000)/60000);},1)\n",
              "    </script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdy8We7XUJSV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "76e36904-2c58-46b3-db60-b670d84f36e8"
      },
      "source": [
        "input_file=urllib.request.urlopen(file_path+file_name)\n",
        "output_file=open(file_name,'wb')\n",
        "output_file.write(input_file.read())\n",
        "output_file.close(); input_file.close()\n",
        "[names,x_train,x_valid,x_test,y_train,y_valid,y_test]=\\\n",
        "h5file2data(file_name,img_size)\n",
        "os.remove(file_name)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file keys: images, labels, names1, names2\n",
            "data outputs: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train</th>\n",
              "      <th>valid</th>\n",
              "      <th>test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image shape</th>\n",
              "      <td>(2837, 64, 64, 3)</td>\n",
              "      <td>(354, 64, 64, 3)</td>\n",
              "      <td>(354, 64, 64, 3)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>image type</th>\n",
              "      <td>float32</td>\n",
              "      <td>float32</td>\n",
              "      <td>float32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label shape</th>\n",
              "      <td>(2, 2837)</td>\n",
              "      <td>(2, 354)</td>\n",
              "      <td>(2, 354)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label type</th>\n",
              "      <td>int32</td>\n",
              "      <td>int32</td>\n",
              "      <td>int32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         train             valid              test\n",
              "image shape  (2837, 64, 64, 3)  (354, 64, 64, 3)  (354, 64, 64, 3)\n",
              "image type             float32           float32           float32\n",
              "label shape          (2, 2837)          (2, 354)          (2, 354)\n",
              "label type               int32             int32             int32"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "distribution of labels: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 756x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "9KXOT8kHuoEx",
        "outputId": "3491c484-a28b-4bf3-b4cf-7e98ace5198e"
      },
      "source": [
        "%cmap_header TF Data Processing"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <head><script src='https://d3js.org/d3.v6.min.js'></script></head>\n",
              "    <style>@import 'https://fonts.googleapis.com/css?family=Smokum&effect=3d'; #colorized {font-family:Smokum; \n",
              "    color:white; padding-left:10px; font-size:30px;}</style>\n",
              "    <h1 id='colorized' class='font-effect-3d'>TF Data Processing</h1>\n",
              "    <script>\n",
              "    var tc=setInterval(function(){\n",
              "        var now=new Date().getTime();\n",
              "        var iddoc=document.getElementById('colorized');\n",
              "        iddoc.style.color=d3.interpolateSinebow(now%(60000)/60000);},1)\n",
              "    </script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AAmDbdPu_D2"
      },
      "source": [
        "batch_size=64\n",
        "train=tf.data.Dataset.from_tensor_slices(\n",
        "    {'input':x_train,\n",
        "     'label':np.array(y_train[1],dtype='float32')}).batch(batch_size)\n",
        "valid=tf.data.Dataset.from_tensor_slices(\n",
        "    {'input':x_valid,\n",
        "     'label':np.array(y_valid[1],dtype='float32')}).batch(batch_size)\n",
        "valid_steps=x_valid.shape[0]//batch_size"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "ZeF3JTJhu_wt",
        "outputId": "06917d34-a4b1-4fbb-d43c-34bc57e6dac0"
      },
      "source": [
        "%cmap_header Models with Adversarial Regularization => MLP"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <head><script src='https://d3js.org/d3.v6.min.js'></script></head>\n",
              "    <style>@import 'https://fonts.googleapis.com/css?family=Smokum&effect=3d'; #colorized {font-family:Smokum; \n",
              "    color:white; padding-left:10px; font-size:30px;}</style>\n",
              "    <h1 id='colorized' class='font-effect-3d'>Models with Adversarial Regularization => MLP</h1>\n",
              "    <script>\n",
              "    var tc=setInterval(function(){\n",
              "        var now=new Date().getTime();\n",
              "        var iddoc=document.getElementById('colorized');\n",
              "        iddoc.style.color=d3.interpolateSinebow(now%(60000)/60000);},1)\n",
              "    </script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRpR8k7CuuTj"
      },
      "source": [
        "num_classes=len(names[1])\n",
        "base_model=tf.keras.Sequential([\n",
        "    tf.keras.Input((img_size,img_size,3),name='input'),\n",
        "    tkl.Flatten(),\n",
        "    tkl.Dense(96,activation=tf.nn.relu),\n",
        "    tkl.BatchNormalization(),    \n",
        "    tkl.Dense(256,activation=tf.nn.relu),\n",
        "    tkl.Dense(num_classes,activation=tf.nn.softmax)\n",
        "])\n",
        "adv_config=nsl.configs.make_adv_reg_config(\n",
        "    multiplier=.2,adv_step_size=.05)\n",
        "adv_model=nsl.keras.AdversarialRegularization(\n",
        "    base_model,adv_config=adv_config)\n",
        "adv_model.compile(optimizer='adam',metrics=['accuracy'],\n",
        "                  loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW8Uf9Rfweas",
        "outputId": "b6fbfb04-07ab-4527-9123-a234a7bbd459"
      },
      "source": [
        "adv_model.fit(train,validation_data=valid,verbose=2,\n",
        "              validation_steps=valid_steps,\n",
        "              epochs=30,callbacks=cb(model_weights));"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc363cd06c8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fc3811b12a0> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Cannot perturb features dict_keys(['label'])WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc363cd06c8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fc3811b12a0> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc363cd06c8>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7fc3811b12a0> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fc37ef8ad08> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fc37ef8ad08> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45/45 - 20s - loss: 2.9966 - sparse_categorical_crossentropy: 2.4941 - sparse_categorical_accuracy: 0.2259 - scaled_adversarial_loss: 0.5025 - val_loss: 6.0288 - val_sparse_categorical_crossentropy: 5.0196 - val_sparse_categorical_accuracy: 0.1375 - val_scaled_adversarial_loss: 1.0092\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7fc37ef8ad08> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "\n",
            "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.13750, saving model to /checkpoints\n",
            "Epoch 2/30\n",
            "45/45 - 2s - loss: 2.4739 - sparse_categorical_crossentropy: 2.0584 - sparse_categorical_accuracy: 0.3761 - scaled_adversarial_loss: 0.4155 - val_loss: 4.6960 - val_sparse_categorical_crossentropy: 3.9082 - val_sparse_categorical_accuracy: 0.1500 - val_scaled_adversarial_loss: 0.7878\n",
            "\n",
            "Epoch 00002: val_sparse_categorical_accuracy improved from 0.13750 to 0.15000, saving model to /checkpoints\n",
            "Epoch 3/30\n",
            "45/45 - 2s - loss: 2.1674 - sparse_categorical_crossentropy: 1.8016 - sparse_categorical_accuracy: 0.4621 - scaled_adversarial_loss: 0.3659 - val_loss: 9.1143 - val_sparse_categorical_crossentropy: 7.5868 - val_sparse_categorical_accuracy: 0.1187 - val_scaled_adversarial_loss: 1.5275\n",
            "\n",
            "Epoch 00003: val_sparse_categorical_accuracy did not improve from 0.15000\n",
            "Epoch 4/30\n",
            "45/45 - 2s - loss: 1.9132 - sparse_categorical_crossentropy: 1.5880 - sparse_categorical_accuracy: 0.5270 - scaled_adversarial_loss: 0.3252 - val_loss: 6.7528 - val_sparse_categorical_crossentropy: 5.6191 - val_sparse_categorical_accuracy: 0.1656 - val_scaled_adversarial_loss: 1.1337\n",
            "\n",
            "Epoch 00004: val_sparse_categorical_accuracy improved from 0.15000 to 0.16563, saving model to /checkpoints\n",
            "Epoch 5/30\n",
            "45/45 - 2s - loss: 1.7140 - sparse_categorical_crossentropy: 1.4204 - sparse_categorical_accuracy: 0.5855 - scaled_adversarial_loss: 0.2937 - val_loss: 7.0665 - val_sparse_categorical_crossentropy: 5.8785 - val_sparse_categorical_accuracy: 0.1375 - val_scaled_adversarial_loss: 1.1879\n",
            "\n",
            "Epoch 00005: val_sparse_categorical_accuracy did not improve from 0.16563\n",
            "Epoch 6/30\n",
            "45/45 - 2s - loss: 1.4955 - sparse_categorical_crossentropy: 1.2367 - sparse_categorical_accuracy: 0.6352 - scaled_adversarial_loss: 0.2588 - val_loss: 4.3453 - val_sparse_categorical_crossentropy: 3.6104 - val_sparse_categorical_accuracy: 0.2250 - val_scaled_adversarial_loss: 0.7349\n",
            "\n",
            "Epoch 00006: val_sparse_categorical_accuracy improved from 0.16563 to 0.22500, saving model to /checkpoints\n",
            "Epoch 7/30\n",
            "45/45 - 2s - loss: 1.2971 - sparse_categorical_crossentropy: 1.0695 - sparse_categorical_accuracy: 0.6940 - scaled_adversarial_loss: 0.2276 - val_loss: 12.6781 - val_sparse_categorical_crossentropy: 10.5470 - val_sparse_categorical_accuracy: 0.0688 - val_scaled_adversarial_loss: 2.1310\n",
            "\n",
            "Epoch 00007: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 8/30\n",
            "45/45 - 2s - loss: 1.1163 - sparse_categorical_crossentropy: 0.9174 - sparse_categorical_accuracy: 0.7409 - scaled_adversarial_loss: 0.1989 - val_loss: 24.6002 - val_sparse_categorical_crossentropy: 20.4775 - val_sparse_categorical_accuracy: 0.0906 - val_scaled_adversarial_loss: 4.1227\n",
            "\n",
            "Epoch 00008: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 9/30\n",
            "45/45 - 2s - loss: 1.0281 - sparse_categorical_crossentropy: 0.8430 - sparse_categorical_accuracy: 0.7624 - scaled_adversarial_loss: 0.1851 - val_loss: 13.3313 - val_sparse_categorical_crossentropy: 11.0753 - val_sparse_categorical_accuracy: 0.0781 - val_scaled_adversarial_loss: 2.2560\n",
            "\n",
            "Epoch 00009: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 10/30\n",
            "45/45 - 2s - loss: 0.8645 - sparse_categorical_crossentropy: 0.7060 - sparse_categorical_accuracy: 0.8044 - scaled_adversarial_loss: 0.1585 - val_loss: 20.7519 - val_sparse_categorical_crossentropy: 17.2631 - val_sparse_categorical_accuracy: 0.0812 - val_scaled_adversarial_loss: 3.4888\n",
            "\n",
            "Epoch 00010: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 11/30\n",
            "45/45 - 2s - loss: 0.7516 - sparse_categorical_crossentropy: 0.6113 - sparse_categorical_accuracy: 0.8294 - scaled_adversarial_loss: 0.1403 - val_loss: 11.3790 - val_sparse_categorical_crossentropy: 9.4529 - val_sparse_categorical_accuracy: 0.1437 - val_scaled_adversarial_loss: 1.9262\n",
            "\n",
            "Epoch 00011: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
            "Epoch 12/30\n",
            "45/45 - 2s - loss: 0.5475 - sparse_categorical_crossentropy: 0.4419 - sparse_categorical_accuracy: 0.8809 - scaled_adversarial_loss: 0.1056 - val_loss: 6.8551 - val_sparse_categorical_crossentropy: 5.6835 - val_sparse_categorical_accuracy: 0.1625 - val_scaled_adversarial_loss: 1.1716\n",
            "\n",
            "Epoch 00012: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 13/30\n",
            "45/45 - 2s - loss: 0.4431 - sparse_categorical_crossentropy: 0.3546 - sparse_categorical_accuracy: 0.9122 - scaled_adversarial_loss: 0.0885 - val_loss: 14.5624 - val_sparse_categorical_crossentropy: 12.0927 - val_sparse_categorical_accuracy: 0.1219 - val_scaled_adversarial_loss: 2.4697\n",
            "\n",
            "Epoch 00013: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 14/30\n",
            "45/45 - 2s - loss: 0.3440 - sparse_categorical_crossentropy: 0.2731 - sparse_categorical_accuracy: 0.9362 - scaled_adversarial_loss: 0.0709 - val_loss: 10.6622 - val_sparse_categorical_crossentropy: 8.8352 - val_sparse_categorical_accuracy: 0.1344 - val_scaled_adversarial_loss: 1.8271\n",
            "\n",
            "Epoch 00014: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 15/30\n",
            "45/45 - 2s - loss: 0.2951 - sparse_categorical_crossentropy: 0.2321 - sparse_categorical_accuracy: 0.9507 - scaled_adversarial_loss: 0.0630 - val_loss: 21.2036 - val_sparse_categorical_crossentropy: 17.6137 - val_sparse_categorical_accuracy: 0.1344 - val_scaled_adversarial_loss: 3.5899\n",
            "\n",
            "Epoch 00015: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 16/30\n",
            "45/45 - 2s - loss: 0.2783 - sparse_categorical_crossentropy: 0.2179 - sparse_categorical_accuracy: 0.9528 - scaled_adversarial_loss: 0.0604 - val_loss: 22.4640 - val_sparse_categorical_crossentropy: 18.6499 - val_sparse_categorical_accuracy: 0.0906 - val_scaled_adversarial_loss: 3.8141\n",
            "\n",
            "Epoch 00016: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
            "Epoch 17/30\n",
            "45/45 - 2s - loss: 0.2045 - sparse_categorical_crossentropy: 0.1585 - sparse_categorical_accuracy: 0.9672 - scaled_adversarial_loss: 0.0460 - val_loss: 19.2777 - val_sparse_categorical_crossentropy: 15.9805 - val_sparse_categorical_accuracy: 0.0781 - val_scaled_adversarial_loss: 3.2972\n",
            "\n",
            "Epoch 00017: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 18/30\n",
            "45/45 - 2s - loss: 0.1798 - sparse_categorical_crossentropy: 0.1382 - sparse_categorical_accuracy: 0.9789 - scaled_adversarial_loss: 0.0415 - val_loss: 20.7741 - val_sparse_categorical_crossentropy: 17.2303 - val_sparse_categorical_accuracy: 0.1344 - val_scaled_adversarial_loss: 3.5438\n",
            "\n",
            "Epoch 00018: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 19/30\n",
            "45/45 - 2s - loss: 0.1609 - sparse_categorical_crossentropy: 0.1232 - sparse_categorical_accuracy: 0.9792 - scaled_adversarial_loss: 0.0376 - val_loss: 27.9356 - val_sparse_categorical_crossentropy: 23.1880 - val_sparse_categorical_accuracy: 0.0719 - val_scaled_adversarial_loss: 4.7476\n",
            "\n",
            "Epoch 00019: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 20/30\n",
            "45/45 - 2s - loss: 0.1597 - sparse_categorical_crossentropy: 0.1219 - sparse_categorical_accuracy: 0.9785 - scaled_adversarial_loss: 0.0379 - val_loss: 33.5280 - val_sparse_categorical_crossentropy: 27.8607 - val_sparse_categorical_accuracy: 0.0938 - val_scaled_adversarial_loss: 5.6673\n",
            "\n",
            "Epoch 00020: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 21/30\n",
            "45/45 - 2s - loss: 0.1710 - sparse_categorical_crossentropy: 0.1308 - sparse_categorical_accuracy: 0.9736 - scaled_adversarial_loss: 0.0401 - val_loss: 24.5469 - val_sparse_categorical_crossentropy: 20.3586 - val_sparse_categorical_accuracy: 0.1000 - val_scaled_adversarial_loss: 4.1883\n",
            "\n",
            "Epoch 00021: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
            "Epoch 22/30\n",
            "45/45 - 2s - loss: 0.1398 - sparse_categorical_crossentropy: 0.1062 - sparse_categorical_accuracy: 0.9820 - scaled_adversarial_loss: 0.0336 - val_loss: 6.8839 - val_sparse_categorical_crossentropy: 5.6719 - val_sparse_categorical_accuracy: 0.2156 - val_scaled_adversarial_loss: 1.2120\n",
            "\n",
            "Epoch 00022: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 23/30\n",
            "45/45 - 2s - loss: 0.0832 - sparse_categorical_crossentropy: 0.0615 - sparse_categorical_accuracy: 0.9926 - scaled_adversarial_loss: 0.0216 - val_loss: 8.4442 - val_sparse_categorical_crossentropy: 6.9731 - val_sparse_categorical_accuracy: 0.1906 - val_scaled_adversarial_loss: 1.4712\n",
            "\n",
            "Epoch 00023: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 24/30\n",
            "45/45 - 2s - loss: 0.0598 - sparse_categorical_crossentropy: 0.0432 - sparse_categorical_accuracy: 0.9947 - scaled_adversarial_loss: 0.0166 - val_loss: 8.7889 - val_sparse_categorical_crossentropy: 7.2590 - val_sparse_categorical_accuracy: 0.1781 - val_scaled_adversarial_loss: 1.5299\n",
            "\n",
            "Epoch 00024: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 25/30\n",
            "45/45 - 2s - loss: 0.0522 - sparse_categorical_crossentropy: 0.0373 - sparse_categorical_accuracy: 0.9968 - scaled_adversarial_loss: 0.0149 - val_loss: 16.7018 - val_sparse_categorical_crossentropy: 13.8099 - val_sparse_categorical_accuracy: 0.0906 - val_scaled_adversarial_loss: 2.8919\n",
            "\n",
            "Epoch 00025: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 26/30\n",
            "45/45 - 2s - loss: 0.0405 - sparse_categorical_crossentropy: 0.0286 - sparse_categorical_accuracy: 0.9972 - scaled_adversarial_loss: 0.0119 - val_loss: 18.0380 - val_sparse_categorical_crossentropy: 14.9182 - val_sparse_categorical_accuracy: 0.0938 - val_scaled_adversarial_loss: 3.1199\n",
            "\n",
            "Epoch 00026: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
            "Epoch 00026: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1_U9Vhj0qV7",
        "outputId": "3ab315ea-d02d-4bdf-d1c1-185bab0ba0e8"
      },
      "source": [
        "adv_model.load_weights(model_weights)\n",
        "dict(zip(adv_model.metrics_names,adv_model.evaluate(\n",
        "    {'input':x_test,\n",
        "     'label':np.array(y_test[1],dtype='float32')},\n",
        "     verbose=0)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 4.450303554534912,\n",
              " 'scaled_adversarial_loss': 0.7525309920310974,\n",
              " 'sparse_categorical_accuracy': 0.1836158186197281,\n",
              " 'sparse_categorical_crossentropy': 3.697772264480591}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "id": "mv8ywHdCnJjt",
        "outputId": "a95f1f8c-baeb-480d-f3e6-813c78288c1c"
      },
      "source": [
        "%cmap_header Models with Adversarial Regularization => CNN"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <head><script src='https://d3js.org/d3.v6.min.js'></script></head>\n",
              "    <style>@import 'https://fonts.googleapis.com/css?family=Smokum&effect=3d'; #colorized {font-family:Smokum; \n",
              "    color:white; padding-left:10px; font-size:30px;}</style>\n",
              "    <h1 id='colorized' class='font-effect-3d'>Models with Adversarial Regularization => CNN</h1>\n",
              "    <script>\n",
              "    var tc=setInterval(function(){\n",
              "        var now=new Date().getTime();\n",
              "        var iddoc=document.getElementById('colorized');\n",
              "        iddoc.style.color=d3.interpolateSinebow(now%(60000)/60000);},1)\n",
              "    </script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDspP2pym4qd"
      },
      "source": [
        "base_model=tf.keras.Sequential([\n",
        "    tf.keras.Input((img_size,img_size,3),name='input'),\n",
        "    tf.keras.layers.Conv2D(32,(5,5),padding='same'),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Dropout(.25),\n",
        "    tf.keras.layers.Conv2D(196,(5,5)),\n",
        "    tf.keras.layers.Activation('relu'),    \n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    tf.keras.layers.Dropout(.25),\n",
        "    tf.keras.layers.GlobalMaxPooling2D(),    \n",
        "    tf.keras.layers.Dense(512),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.Dropout(.25),\n",
        "    tf.keras.layers.Dense(128),\n",
        "    tf.keras.layers.Activation('relu'),\n",
        "    tf.keras.layers.Dropout(.25),\n",
        "    tf.keras.layers.Dense(num_classes,activation='softmax')\n",
        "])\n",
        "adv_config=nsl.configs\\\n",
        ".make_adv_reg_config(multiplier=.2,adv_step_size=.05)\n",
        "adv_model=nsl.keras\\\n",
        ".AdversarialRegularization(base_model,adv_config=adv_config)\n",
        "adv_model.compile(optimizer='adam',metrics=['accuracy'],\n",
        "                  loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P42mWexJnr_a",
        "outputId": "c6d29435-96fb-49be-f6b7-bb0d70c6b7f0"
      },
      "source": [
        "adv_model.fit(train,validation_data=valid,verbose=2,\n",
        "              validation_steps=valid_steps,\n",
        "              epochs=30,callbacks=cb(model_weights));"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "45/45 - 115s - loss: 3.3158 - sparse_categorical_crossentropy: 2.7633 - sparse_categorical_accuracy: 0.0800 - scaled_adversarial_loss: 0.5525 - val_loss: 3.2856 - val_sparse_categorical_crossentropy: 2.7378 - val_sparse_categorical_accuracy: 0.1031 - val_scaled_adversarial_loss: 0.5478\n",
            "\n",
            "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.10312, saving model to /checkpoints\n",
            "Epoch 2/30\n",
            "45/45 - 113s - loss: 3.2862 - sparse_categorical_crossentropy: 2.7390 - sparse_categorical_accuracy: 0.0730 - scaled_adversarial_loss: 0.5472 - val_loss: 3.2380 - val_sparse_categorical_crossentropy: 2.6979 - val_sparse_categorical_accuracy: 0.0719 - val_scaled_adversarial_loss: 0.5402\n",
            "\n",
            "Epoch 00002: val_sparse_categorical_accuracy did not improve from 0.10312\n",
            "Epoch 3/30\n",
            "45/45 - 114s - loss: 3.2348 - sparse_categorical_crossentropy: 2.6948 - sparse_categorical_accuracy: 0.0973 - scaled_adversarial_loss: 0.5400 - val_loss: 3.1885 - val_sparse_categorical_crossentropy: 2.6559 - val_sparse_categorical_accuracy: 0.0938 - val_scaled_adversarial_loss: 0.5326\n",
            "\n",
            "Epoch 00003: val_sparse_categorical_accuracy did not improve from 0.10312\n",
            "Epoch 4/30\n",
            "45/45 - 114s - loss: 3.1296 - sparse_categorical_crossentropy: 2.6068 - sparse_categorical_accuracy: 0.1466 - scaled_adversarial_loss: 0.5229 - val_loss: 3.1016 - val_sparse_categorical_crossentropy: 2.5824 - val_sparse_categorical_accuracy: 0.1094 - val_scaled_adversarial_loss: 0.5193\n",
            "\n",
            "Epoch 00004: val_sparse_categorical_accuracy improved from 0.10312 to 0.10938, saving model to /checkpoints\n",
            "Epoch 5/30\n",
            "45/45 - 114s - loss: 3.0217 - sparse_categorical_crossentropy: 2.5165 - sparse_categorical_accuracy: 0.1569 - scaled_adversarial_loss: 0.5052 - val_loss: 2.9409 - val_sparse_categorical_crossentropy: 2.4474 - val_sparse_categorical_accuracy: 0.1625 - val_scaled_adversarial_loss: 0.4935\n",
            "\n",
            "Epoch 00005: val_sparse_categorical_accuracy improved from 0.10938 to 0.16250, saving model to /checkpoints\n",
            "Epoch 6/30\n",
            "45/45 - 115s - loss: 2.8606 - sparse_categorical_crossentropy: 2.3828 - sparse_categorical_accuracy: 0.1861 - scaled_adversarial_loss: 0.4778 - val_loss: 2.8154 - val_sparse_categorical_crossentropy: 2.3424 - val_sparse_categorical_accuracy: 0.2250 - val_scaled_adversarial_loss: 0.4731\n",
            "\n",
            "Epoch 00006: val_sparse_categorical_accuracy improved from 0.16250 to 0.22500, saving model to /checkpoints\n",
            "Epoch 7/30\n",
            "45/45 - 113s - loss: 2.7161 - sparse_categorical_crossentropy: 2.2579 - sparse_categorical_accuracy: 0.2316 - scaled_adversarial_loss: 0.4582 - val_loss: 2.7419 - val_sparse_categorical_crossentropy: 2.2801 - val_sparse_categorical_accuracy: 0.2188 - val_scaled_adversarial_loss: 0.4618\n",
            "\n",
            "Epoch 00007: val_sparse_categorical_accuracy did not improve from 0.22500\n",
            "Epoch 8/30\n",
            "45/45 - 114s - loss: 2.5518 - sparse_categorical_crossentropy: 2.1226 - sparse_categorical_accuracy: 0.2686 - scaled_adversarial_loss: 0.4292 - val_loss: 2.6404 - val_sparse_categorical_crossentropy: 2.1953 - val_sparse_categorical_accuracy: 0.2375 - val_scaled_adversarial_loss: 0.4451\n",
            "\n",
            "Epoch 00008: val_sparse_categorical_accuracy improved from 0.22500 to 0.23750, saving model to /checkpoints\n",
            "Epoch 9/30\n",
            "45/45 - 113s - loss: 2.4188 - sparse_categorical_crossentropy: 2.0136 - sparse_categorical_accuracy: 0.2971 - scaled_adversarial_loss: 0.4052 - val_loss: 2.5454 - val_sparse_categorical_crossentropy: 2.1151 - val_sparse_categorical_accuracy: 0.2937 - val_scaled_adversarial_loss: 0.4303\n",
            "\n",
            "Epoch 00009: val_sparse_categorical_accuracy improved from 0.23750 to 0.29375, saving model to /checkpoints\n",
            "Epoch 10/30\n",
            "45/45 - 112s - loss: 2.3040 - sparse_categorical_crossentropy: 1.9161 - sparse_categorical_accuracy: 0.3363 - scaled_adversarial_loss: 0.3879 - val_loss: 2.4935 - val_sparse_categorical_crossentropy: 2.0712 - val_sparse_categorical_accuracy: 0.2688 - val_scaled_adversarial_loss: 0.4224\n",
            "\n",
            "Epoch 00010: val_sparse_categorical_accuracy did not improve from 0.29375\n",
            "Epoch 11/30\n",
            "45/45 - 113s - loss: 2.2461 - sparse_categorical_crossentropy: 1.8699 - sparse_categorical_accuracy: 0.3553 - scaled_adversarial_loss: 0.3762 - val_loss: 2.4345 - val_sparse_categorical_crossentropy: 2.0211 - val_sparse_categorical_accuracy: 0.2906 - val_scaled_adversarial_loss: 0.4135\n",
            "\n",
            "Epoch 00011: val_sparse_categorical_accuracy did not improve from 0.29375\n",
            "Epoch 12/30\n",
            "45/45 - 113s - loss: 2.1350 - sparse_categorical_crossentropy: 1.7758 - sparse_categorical_accuracy: 0.3789 - scaled_adversarial_loss: 0.3592 - val_loss: 2.4317 - val_sparse_categorical_crossentropy: 2.0181 - val_sparse_categorical_accuracy: 0.3625 - val_scaled_adversarial_loss: 0.4136\n",
            "\n",
            "Epoch 00012: val_sparse_categorical_accuracy improved from 0.29375 to 0.36250, saving model to /checkpoints\n",
            "Epoch 13/30\n",
            "45/45 - 112s - loss: 2.0627 - sparse_categorical_crossentropy: 1.7135 - sparse_categorical_accuracy: 0.4001 - scaled_adversarial_loss: 0.3491 - val_loss: 2.4193 - val_sparse_categorical_crossentropy: 2.0076 - val_sparse_categorical_accuracy: 0.3625 - val_scaled_adversarial_loss: 0.4117\n",
            "\n",
            "Epoch 00013: val_sparse_categorical_accuracy did not improve from 0.36250\n",
            "Epoch 14/30\n",
            "45/45 - 112s - loss: 1.9501 - sparse_categorical_crossentropy: 1.6228 - sparse_categorical_accuracy: 0.4346 - scaled_adversarial_loss: 0.3274 - val_loss: 2.1900 - val_sparse_categorical_crossentropy: 1.8153 - val_sparse_categorical_accuracy: 0.4250 - val_scaled_adversarial_loss: 0.3747\n",
            "\n",
            "Epoch 00014: val_sparse_categorical_accuracy improved from 0.36250 to 0.42500, saving model to /checkpoints\n",
            "Epoch 15/30\n",
            "45/45 - 112s - loss: 1.8538 - sparse_categorical_crossentropy: 1.5392 - sparse_categorical_accuracy: 0.4720 - scaled_adversarial_loss: 0.3146 - val_loss: 2.1776 - val_sparse_categorical_crossentropy: 1.8034 - val_sparse_categorical_accuracy: 0.4062 - val_scaled_adversarial_loss: 0.3742\n",
            "\n",
            "Epoch 00015: val_sparse_categorical_accuracy did not improve from 0.42500\n",
            "Epoch 16/30\n",
            "45/45 - 112s - loss: 1.8045 - sparse_categorical_crossentropy: 1.4989 - sparse_categorical_accuracy: 0.4766 - scaled_adversarial_loss: 0.3056 - val_loss: 2.1464 - val_sparse_categorical_crossentropy: 1.7775 - val_sparse_categorical_accuracy: 0.4281 - val_scaled_adversarial_loss: 0.3689\n",
            "\n",
            "Epoch 00016: val_sparse_categorical_accuracy improved from 0.42500 to 0.42812, saving model to /checkpoints\n",
            "Epoch 17/30\n",
            "45/45 - 113s - loss: 1.6753 - sparse_categorical_crossentropy: 1.3862 - sparse_categorical_accuracy: 0.5196 - scaled_adversarial_loss: 0.2890 - val_loss: 2.1011 - val_sparse_categorical_crossentropy: 1.7388 - val_sparse_categorical_accuracy: 0.4219 - val_scaled_adversarial_loss: 0.3623\n",
            "\n",
            "Epoch 00017: val_sparse_categorical_accuracy did not improve from 0.42812\n",
            "Epoch 18/30\n",
            "45/45 - 112s - loss: 1.5834 - sparse_categorical_crossentropy: 1.3081 - sparse_categorical_accuracy: 0.5411 - scaled_adversarial_loss: 0.2753 - val_loss: 2.0690 - val_sparse_categorical_crossentropy: 1.7105 - val_sparse_categorical_accuracy: 0.4781 - val_scaled_adversarial_loss: 0.3585\n",
            "\n",
            "Epoch 00018: val_sparse_categorical_accuracy improved from 0.42812 to 0.47813, saving model to /checkpoints\n",
            "Epoch 19/30\n",
            "45/45 - 113s - loss: 1.5527 - sparse_categorical_crossentropy: 1.2895 - sparse_categorical_accuracy: 0.5569 - scaled_adversarial_loss: 0.2632 - val_loss: 2.0114 - val_sparse_categorical_crossentropy: 1.6628 - val_sparse_categorical_accuracy: 0.4688 - val_scaled_adversarial_loss: 0.3486\n",
            "\n",
            "Epoch 00019: val_sparse_categorical_accuracy did not improve from 0.47813\n",
            "Epoch 20/30\n",
            "45/45 - 114s - loss: 1.4828 - sparse_categorical_crossentropy: 1.2310 - sparse_categorical_accuracy: 0.5654 - scaled_adversarial_loss: 0.2517 - val_loss: 2.0243 - val_sparse_categorical_crossentropy: 1.6727 - val_sparse_categorical_accuracy: 0.4344 - val_scaled_adversarial_loss: 0.3516\n",
            "\n",
            "Epoch 00020: val_sparse_categorical_accuracy did not improve from 0.47813\n",
            "Epoch 21/30\n",
            "45/45 - 111s - loss: 1.3870 - sparse_categorical_crossentropy: 1.1506 - sparse_categorical_accuracy: 0.6006 - scaled_adversarial_loss: 0.2364 - val_loss: 1.8566 - val_sparse_categorical_crossentropy: 1.5329 - val_sparse_categorical_accuracy: 0.5281 - val_scaled_adversarial_loss: 0.3237\n",
            "\n",
            "Epoch 00021: val_sparse_categorical_accuracy improved from 0.47813 to 0.52812, saving model to /checkpoints\n",
            "Epoch 22/30\n",
            "45/45 - 112s - loss: 1.3592 - sparse_categorical_crossentropy: 1.1272 - sparse_categorical_accuracy: 0.6052 - scaled_adversarial_loss: 0.2320 - val_loss: 1.8963 - val_sparse_categorical_crossentropy: 1.5644 - val_sparse_categorical_accuracy: 0.5000 - val_scaled_adversarial_loss: 0.3319\n",
            "\n",
            "Epoch 00022: val_sparse_categorical_accuracy did not improve from 0.52812\n",
            "Epoch 23/30\n",
            "45/45 - 112s - loss: 1.3397 - sparse_categorical_crossentropy: 1.1115 - sparse_categorical_accuracy: 0.6087 - scaled_adversarial_loss: 0.2281 - val_loss: 1.9014 - val_sparse_categorical_crossentropy: 1.5686 - val_sparse_categorical_accuracy: 0.5094 - val_scaled_adversarial_loss: 0.3328\n",
            "\n",
            "Epoch 00023: val_sparse_categorical_accuracy did not improve from 0.52812\n",
            "Epoch 24/30\n",
            "45/45 - 112s - loss: 1.2877 - sparse_categorical_crossentropy: 1.0691 - sparse_categorical_accuracy: 0.6260 - scaled_adversarial_loss: 0.2185 - val_loss: 1.8241 - val_sparse_categorical_crossentropy: 1.5038 - val_sparse_categorical_accuracy: 0.5125 - val_scaled_adversarial_loss: 0.3202\n",
            "\n",
            "Epoch 00024: val_sparse_categorical_accuracy did not improve from 0.52812\n",
            "Epoch 25/30\n",
            "45/45 - 112s - loss: 1.2173 - sparse_categorical_crossentropy: 1.0079 - sparse_categorical_accuracy: 0.6475 - scaled_adversarial_loss: 0.2093 - val_loss: 1.8192 - val_sparse_categorical_crossentropy: 1.4992 - val_sparse_categorical_accuracy: 0.5094 - val_scaled_adversarial_loss: 0.3200\n",
            "\n",
            "Epoch 00025: val_sparse_categorical_accuracy did not improve from 0.52812\n",
            "Epoch 26/30\n",
            "45/45 - 112s - loss: 1.1922 - sparse_categorical_crossentropy: 0.9887 - sparse_categorical_accuracy: 0.6588 - scaled_adversarial_loss: 0.2035 - val_loss: 1.7273 - val_sparse_categorical_crossentropy: 1.4211 - val_sparse_categorical_accuracy: 0.5437 - val_scaled_adversarial_loss: 0.3062\n",
            "\n",
            "Epoch 00026: val_sparse_categorical_accuracy improved from 0.52812 to 0.54375, saving model to /checkpoints\n",
            "Epoch 27/30\n",
            "45/45 - 112s - loss: 1.1451 - sparse_categorical_crossentropy: 0.9543 - sparse_categorical_accuracy: 0.6560 - scaled_adversarial_loss: 0.1908 - val_loss: 1.8025 - val_sparse_categorical_crossentropy: 1.4834 - val_sparse_categorical_accuracy: 0.5312 - val_scaled_adversarial_loss: 0.3191\n",
            "\n",
            "Epoch 00027: val_sparse_categorical_accuracy did not improve from 0.54375\n",
            "Epoch 28/30\n",
            "45/45 - 113s - loss: 1.1104 - sparse_categorical_crossentropy: 0.9197 - sparse_categorical_accuracy: 0.6729 - scaled_adversarial_loss: 0.1907 - val_loss: 1.8278 - val_sparse_categorical_crossentropy: 1.5035 - val_sparse_categorical_accuracy: 0.5281 - val_scaled_adversarial_loss: 0.3242\n",
            "\n",
            "Epoch 00028: val_sparse_categorical_accuracy did not improve from 0.54375\n",
            "Epoch 29/30\n",
            "45/45 - 111s - loss: 1.0612 - sparse_categorical_crossentropy: 0.8768 - sparse_categorical_accuracy: 0.6909 - scaled_adversarial_loss: 0.1844 - val_loss: 1.6404 - val_sparse_categorical_crossentropy: 1.3472 - val_sparse_categorical_accuracy: 0.5531 - val_scaled_adversarial_loss: 0.2931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsRQQp0kn7vS",
        "outputId": "3eb7b56c-54ac-4d6e-da68-4370a23eec97"
      },
      "source": [
        "adv_model.load_weights(model_weights)\n",
        "dict(zip(adv_model.metrics_names,adv_model.evaluate(\n",
        "    {'input':x_test,\n",
        "     'label':np.array(y_test[1],dtype='float32')},\n",
        "     verbose=0)))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 1.6004232168197632,\n",
              " 'scaled_adversarial_loss': 0.2867552638053894,\n",
              " 'sparse_categorical_accuracy': 0.5282486081123352,\n",
              " 'sparse_categorical_crossentropy': 1.313667893409729}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}