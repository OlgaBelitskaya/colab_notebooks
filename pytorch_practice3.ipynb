{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch_practice3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOZQxDi5dYKbz1pGkB9EdKs"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sesPHYI2EMF4","colab_type":"text"},"source":["## Code Modules & Functions"]},{"cell_type":"code","metadata":{"id":"aJhQT1T6CyPG","colab_type":"code","colab":{}},"source":["import numpy as np,pandas as pd,pylab as pl\n","import h5py,torch,urllib,zipfile\n","from torchvision.datasets import MNIST as tmnist\n","from torchvision import transforms\n","from torch.utils.data import DataLoader as tdl\n","from torch.utils.data import Dataset as tds\n","import torch.nn.functional as tnnf\n","from sklearn.datasets import make_classification\n","from IPython.core.magic import register_line_magic\n","dev=torch.device(\"cuda:0\" if torch.cuda.is_available() \n","                 else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Hj1Fi-OEbp3","colab_type":"code","colab":{}},"source":["def model_acc(model,data_loader,num_features):\n","    correct_preds,num_examples=0,0    \n","    for features,targets in data_loader:\n","        features=features.view(-1,num_features).to(dev)\n","        targets=targets.to(dev)\n","        logits,probs=model(features)\n","        _,pred_labels=torch.max(probs,1)\n","        num_examples+=targets.size(0)\n","        correct_preds+=(pred_labels==targets).sum()        \n","    return correct_preds.float()/num_examples*100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8TpqGh4Eg7K","colab_type":"code","colab":{}},"source":["@register_line_magic\n","def print_acc(t):\n","    if t=='test':\n","        print('Test accuracy: %.4f%%'%\\\n","        (model_acc(model,test_loader,num_features)))\n","    if t=='train':\n","        print('Train accuracy: %.4f%%'%\\\n","        (model_acc(model,train_loader,num_features)))\n","@register_line_magic\n","def print_acc2(t):\n","    if t=='test':\n","        print('Test accuracy: %.4f%%'%\\\n","        (model_acc(model,test_loader2,num_features2)))\n","    if t=='train':\n","        print('Train accuracy: %.4f%%'%\\\n","        (model_acc(model,train_loader2,num_features2)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTeYzbqLGSJy","colab_type":"code","colab":{}},"source":["@register_line_magic\n","def train_run(epochs):\n","    epochs=int(epochs)\n","    for epoch in range(epochs):\n","        for batch_ids,(features,targets) in enumerate(train_loader):        \n","            features=features.view(-1,num_features).to(dev)\n","            targets=targets.to(dev)\n","            logits,probs=model(features)\n","            cost=tnnf.cross_entropy(logits,targets)\n","            optimizer.zero_grad(); cost.backward()\n","            optimizer.step()\n","            if not batch_ids%300:\n","                print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n","                       %(epoch+1,epochs,batch_ids, \n","                         len(train)//batch_size,cost))           \n","        with torch.set_grad_enabled(False):\n","            print('Epoch: %03d/%03d train accuracy: %.2f%%'%\\\n","                  (epoch+1,epochs,model_acc(model,train_loader,\n","                                            num_features)))\n","@register_line_magic\n","def train_run2(epochs):\n","    epochs=int(epochs)\n","    for epoch in range(epochs):\n","        for batch_ids,(features,targets) in enumerate(train_loader2):        \n","            features=features.view(-1,num_features2).to(dev)\n","            targets=targets.to(dev)\n","            logits,probs=model(features)\n","            cost=tnnf.cross_entropy(logits,targets.long())\n","            optimizer.zero_grad(); cost.backward()\n","            optimizer.step()\n","            if not batch_ids%300:\n","                print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n","                       %(epoch+1,epochs,batch_ids, \n","                         len(train2)//batch_size2,cost))           \n","        with torch.set_grad_enabled(False):\n","            print('Epoch: %03d/%03d train accuracy: %.2f%%'%\\\n","                  (epoch+1,epochs,model_acc(model,train_loader2,\n","                                            num_features2)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HVC2AyonHZDs","colab_type":"text"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"m0vNdKqYHbC6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"13b89c06-b959-46be-8020-39447b47c2b9","executionInfo":{"status":"ok","timestamp":1591184209492,"user_tz":-180,"elapsed":1794,"user":{"displayName":"Olga Safu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhITqyZukwHZ9teMEwtxrx1LXmu7BQL_S_bK8qJFLU=s64","userId":"13149748190150435632"}}},"source":["random_seed=1; batch_size=64\n","train=tmnist(root='data',train=True,download=True,\n","            transform=transforms.ToTensor())\n","test=tmnist(root='data',train=False, \n","            transform=transforms.ToTensor())\n","train_loader=tdl(dataset=train,shuffle=True, \n","                 batch_size=batch_size)\n","test_loader=tdl(dataset=test,shuffle=False, \n","                batch_size=batch_size)\n","for images,labels in train_loader:  \n","    print('Image dimensions: %s'%str(images.shape))\n","    print('Label dimensions: %s'%str(labels.shape))\n","    break"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Image dimensions: torch.Size([64, 1, 28, 28])\n","Label dimensions: torch.Size([64])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4B2RoMLOgect","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"ecf26487-fb90-492c-a3b9-364fa37b1d2c","executionInfo":{"status":"ok","timestamp":1591185773354,"user_tz":-180,"elapsed":25520,"user":{"displayName":"Olga Safu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhITqyZukwHZ9teMEwtxrx1LXmu7BQL_S_bK8qJFLU=s64","userId":"13149748190150435632"}}},"source":["fpath='https://olgabelitskaya.github.io/'\n","zf='LetterColorImages_123.h5.zip'\n","input_file=urllib.request.urlopen(fpath+zf)\n","output_file=open(zf,'wb'); \n","output_file.write(input_file.read())\n","output_file.close(); input_file.close()\n","zipf=zipfile.ZipFile(zf,'r')\n","zipf.extractall(''); zipf.close()\n","f=h5py.File(zf[:-4],'r')\n","keys=list(f.keys()); print(keys)\n","X=np.array(f[keys[1]],dtype='float32')/255\n","y=np.array(f[keys[2]],dtype='int32')-1\n","N=len(y); n=int(.2*N); batch_size=16\n","shuffle_ids=np.arange(N)\n","np.random.RandomState(23).shuffle(shuffle_ids)\n","X,y=X[shuffle_ids],y[shuffle_ids]\n","X_test,X_train=X[:n],X[n:]\n","y_test,y_train=y[:n],y[n:]\n","X_train.shape,y_train.shape"],"execution_count":13,"outputs":[{"output_type":"stream","text":["['backgrounds', 'images', 'labels']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["((11352, 32, 32, 3), (11352,))"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"GxIeSo4FECR0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"ce26089d-49fa-4afc-a002-f26b6c9bdff6","executionInfo":{"status":"ok","timestamp":1591185785519,"user_tz":-180,"elapsed":1294,"user":{"displayName":"Olga Safu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhITqyZukwHZ9teMEwtxrx1LXmu7BQL_S_bK8qJFLU=s64","userId":"13149748190150435632"}}},"source":["random_seed=1; batch_size2=64\n","class TData(tds):\n","    def __init__(self,X,y):   \n","        self.X=torch.tensor(X,dtype=torch.float32)\n","        self.y=torch.tensor(y,dtype=torch.int32)\n","    def __getitem__(self,index):\n","        train_img,train_lbl=self.X[index],self.y[index]\n","        return train_img,train_lbl\n","    def __len__(self):\n","        return self.y.shape[0]\n","train2=TData(X_train,y_train)\n","test2=TData(X_test,y_test)\n","train_loader2=tdl(dataset=train2,batch_size=batch_size2,shuffle=True)\n","test_loader2=tdl(dataset=test2,batch_size=batch_size2,shuffle=False)\n","for images,labels in train_loader2:  \n","    print('Image dimensions: %s'%str(images.shape))\n","    print('Label dimensions: %s'%str(labels.shape))\n","    break"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Image dimensions: torch.Size([64, 32, 32, 3])\n","Label dimensions: torch.Size([64])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KTCMMW6cEFns","colab_type":"text"},"source":["## MLP with BatchNormalization"]},{"cell_type":"code","metadata":{"id":"gfpCKg4EEQ6i","colab_type":"code","colab":{}},"source":["num_features=784; num_classes=10\n","hidden1=512; hidden2=256; hidden3=128\n","class MLPN(torch.nn.Module):\n","    def __init__(self,num_features,num_classes):\n","        super(MLPN,self).__init__()\n","        self.linear1=torch.nn.Linear(num_features,hidden1)\n","        self.linear1.weight.detach().normal_(0.,.1)\n","        self.linear1.bias.detach().zero_()\n","        self.linear1bn=torch.nn.BatchNorm1d(hidden1)\n","        self.linear2=torch.nn.Linear(hidden1,hidden2)\n","        self.linear2.weight.detach().normal_(0.,.1)\n","        self.linear2.bias.detach().zero_()\n","        self.linear2bn=torch.nn.BatchNorm1d(hidden2)\n","        self.linear3=torch.nn.Linear(hidden2,hidden3)\n","        self.linear3.weight.detach().normal_(0.,.1)\n","        self.linear3.bias.detach().zero_()\n","        self.linear3bn=torch.nn.BatchNorm1d(hidden3)\n","        self.linear_out=torch.nn.Linear(hidden3,num_classes)\n","        self.linear_out.weight.detach().normal_(0.,.1)\n","        self.linear_out.bias.detach().zero_() \n","    def forward(self,x):\n","        y=self.linear1(x); y=self.linear1bn(y)\n","        y=tnnf.relu(y)\n","        y=self.linear2(y); y=self.linear2bn(y)\n","        y=tnnf.relu(y)\n","        y=self.linear3(y); y=self.linear3bn(y)\n","        y=tnnf.relu(y)\n","        logits=self.linear_out(y)\n","        probs=tnnf.log_softmax(logits,dim=1)\n","        return logits,probs   \n","torch.manual_seed(random_seed)\n","model=MLPN(num_features=num_features,\n","           num_classes=num_classes)\n","model=model.to(dev); learning_rate=.01\n","optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CrDKtZGyFUMI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b91eea93-b57b-42f8-816c-a72c3ae32b2f","executionInfo":{"status":"ok","timestamp":1591186789549,"user_tz":-180,"elapsed":650593,"user":{"displayName":"Olga Safu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhITqyZukwHZ9teMEwtxrx1LXmu7BQL_S_bK8qJFLU=s64","userId":"13149748190150435632"}}},"source":["%train_run 20"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch: 001/020 | Batch 000/3750 | Cost: 2.6883\n","Epoch: 001/020 | Batch 300/3750 | Cost: 0.8777\n","Epoch: 001/020 | Batch 600/3750 | Cost: 0.6617\n","Epoch: 001/020 | Batch 900/3750 | Cost: 0.4860\n","Epoch: 001/020 train accuracy: 88.61%\n","Epoch: 002/020 | Batch 000/3750 | Cost: 0.5334\n","Epoch: 002/020 | Batch 300/3750 | Cost: 0.2812\n","Epoch: 002/020 | Batch 600/3750 | Cost: 0.2545\n","Epoch: 002/020 | Batch 900/3750 | Cost: 0.3162\n","Epoch: 002/020 train accuracy: 91.70%\n","Epoch: 003/020 | Batch 000/3750 | Cost: 0.2939\n","Epoch: 003/020 | Batch 300/3750 | Cost: 0.4081\n","Epoch: 003/020 | Batch 600/3750 | Cost: 0.3482\n","Epoch: 003/020 | Batch 900/3750 | Cost: 0.2484\n","Epoch: 003/020 train accuracy: 93.36%\n","Epoch: 004/020 | Batch 000/3750 | Cost: 0.1364\n","Epoch: 004/020 | Batch 300/3750 | Cost: 0.2304\n","Epoch: 004/020 | Batch 600/3750 | Cost: 0.1658\n","Epoch: 004/020 | Batch 900/3750 | Cost: 0.1506\n","Epoch: 004/020 train accuracy: 94.25%\n","Epoch: 005/020 | Batch 000/3750 | Cost: 0.1944\n","Epoch: 005/020 | Batch 300/3750 | Cost: 0.1668\n","Epoch: 005/020 | Batch 600/3750 | Cost: 0.3775\n","Epoch: 005/020 | Batch 900/3750 | Cost: 0.2059\n","Epoch: 005/020 train accuracy: 94.96%\n","Epoch: 006/020 | Batch 000/3750 | Cost: 0.2097\n","Epoch: 006/020 | Batch 300/3750 | Cost: 0.0812\n","Epoch: 006/020 | Batch 600/3750 | Cost: 0.1646\n","Epoch: 006/020 | Batch 900/3750 | Cost: 0.1541\n","Epoch: 006/020 train accuracy: 95.48%\n","Epoch: 007/020 | Batch 000/3750 | Cost: 0.1777\n","Epoch: 007/020 | Batch 300/3750 | Cost: 0.1626\n","Epoch: 007/020 | Batch 600/3750 | Cost: 0.0808\n","Epoch: 007/020 | Batch 900/3750 | Cost: 0.1672\n","Epoch: 007/020 train accuracy: 95.92%\n","Epoch: 008/020 | Batch 000/3750 | Cost: 0.2306\n","Epoch: 008/020 | Batch 300/3750 | Cost: 0.1271\n","Epoch: 008/020 | Batch 600/3750 | Cost: 0.1450\n","Epoch: 008/020 | Batch 900/3750 | Cost: 0.1061\n","Epoch: 008/020 train accuracy: 96.30%\n","Epoch: 009/020 | Batch 000/3750 | Cost: 0.2846\n","Epoch: 009/020 | Batch 300/3750 | Cost: 0.0866\n","Epoch: 009/020 | Batch 600/3750 | Cost: 0.1278\n","Epoch: 009/020 | Batch 900/3750 | Cost: 0.0831\n","Epoch: 009/020 train accuracy: 96.57%\n","Epoch: 010/020 | Batch 000/3750 | Cost: 0.0727\n","Epoch: 010/020 | Batch 300/3750 | Cost: 0.1605\n","Epoch: 010/020 | Batch 600/3750 | Cost: 0.0663\n","Epoch: 010/020 | Batch 900/3750 | Cost: 0.0495\n","Epoch: 010/020 train accuracy: 96.87%\n","Epoch: 011/020 | Batch 000/3750 | Cost: 0.0916\n","Epoch: 011/020 | Batch 300/3750 | Cost: 0.2150\n","Epoch: 011/020 | Batch 600/3750 | Cost: 0.0314\n","Epoch: 011/020 | Batch 900/3750 | Cost: 0.1596\n","Epoch: 011/020 train accuracy: 97.06%\n","Epoch: 012/020 | Batch 000/3750 | Cost: 0.1585\n","Epoch: 012/020 | Batch 300/3750 | Cost: 0.0349\n","Epoch: 012/020 | Batch 600/3750 | Cost: 0.1922\n","Epoch: 012/020 | Batch 900/3750 | Cost: 0.0485\n","Epoch: 012/020 train accuracy: 97.29%\n","Epoch: 013/020 | Batch 000/3750 | Cost: 0.0827\n","Epoch: 013/020 | Batch 300/3750 | Cost: 0.0954\n","Epoch: 013/020 | Batch 600/3750 | Cost: 0.0495\n","Epoch: 013/020 | Batch 900/3750 | Cost: 0.1636\n","Epoch: 013/020 train accuracy: 97.47%\n","Epoch: 014/020 | Batch 000/3750 | Cost: 0.0456\n","Epoch: 014/020 | Batch 300/3750 | Cost: 0.0896\n","Epoch: 014/020 | Batch 600/3750 | Cost: 0.1199\n","Epoch: 014/020 | Batch 900/3750 | Cost: 0.0754\n","Epoch: 014/020 train accuracy: 97.60%\n","Epoch: 015/020 | Batch 000/3750 | Cost: 0.0365\n","Epoch: 015/020 | Batch 300/3750 | Cost: 0.0512\n","Epoch: 015/020 | Batch 600/3750 | Cost: 0.0904\n","Epoch: 015/020 | Batch 900/3750 | Cost: 0.0992\n","Epoch: 015/020 train accuracy: 97.79%\n","Epoch: 016/020 | Batch 000/3750 | Cost: 0.0915\n","Epoch: 016/020 | Batch 300/3750 | Cost: 0.0372\n","Epoch: 016/020 | Batch 600/3750 | Cost: 0.0539\n","Epoch: 016/020 | Batch 900/3750 | Cost: 0.1950\n","Epoch: 016/020 train accuracy: 97.93%\n","Epoch: 017/020 | Batch 000/3750 | Cost: 0.0555\n","Epoch: 017/020 | Batch 300/3750 | Cost: 0.0210\n","Epoch: 017/020 | Batch 600/3750 | Cost: 0.0544\n","Epoch: 017/020 | Batch 900/3750 | Cost: 0.0337\n","Epoch: 017/020 train accuracy: 98.15%\n","Epoch: 018/020 | Batch 000/3750 | Cost: 0.0395\n","Epoch: 018/020 | Batch 300/3750 | Cost: 0.0362\n","Epoch: 018/020 | Batch 600/3750 | Cost: 0.0425\n","Epoch: 018/020 | Batch 900/3750 | Cost: 0.0575\n","Epoch: 018/020 train accuracy: 98.20%\n","Epoch: 019/020 | Batch 000/3750 | Cost: 0.0533\n","Epoch: 019/020 | Batch 300/3750 | Cost: 0.0160\n","Epoch: 019/020 | Batch 600/3750 | Cost: 0.0678\n","Epoch: 019/020 | Batch 900/3750 | Cost: 0.0548\n","Epoch: 019/020 train accuracy: 98.40%\n","Epoch: 020/020 | Batch 000/3750 | Cost: 0.0738\n","Epoch: 020/020 | Batch 300/3750 | Cost: 0.0473\n","Epoch: 020/020 | Batch 600/3750 | Cost: 0.0538\n","Epoch: 020/020 | Batch 900/3750 | Cost: 0.0257\n","Epoch: 020/020 train accuracy: 98.42%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h9R_lgB0Fbol","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"90fcca29-8563-409e-87b0-a91b7c4cfd8c","executionInfo":{"status":"ok","timestamp":1591186805449,"user_tz":-180,"elapsed":15869,"user":{"displayName":"Olga Safu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhITqyZukwHZ9teMEwtxrx1LXmu7BQL_S_bK8qJFLU=s64","userId":"13149748190150435632"}}},"source":["%print_acc train\n","%print_acc test"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Train accuracy: 98.4433%\n","Test accuracy: 96.6800%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vl0fmrtXFusi","colab_type":"code","colab":{}},"source":["num_features2=32*32*3; num_classes2=33\n","hidden1=1024; hidden2=256; hidden3=256\n","class MLPN2(torch.nn.Module):\n","    def __init__(self,num_features,num_classes):\n","        super(MLPN2,self).__init__()\n","        self.linear1=torch.nn.Linear(num_features,hidden1)\n","        self.linear1.weight.detach().normal_(0.,.1)\n","        self.linear1.bias.detach().zero_()\n","        self.linear1bn=torch.nn.BatchNorm1d(hidden1)\n","        self.linear2=torch.nn.Linear(hidden1,hidden2)\n","        self.linear2.weight.detach().normal_(0.,.1)\n","        self.linear2.bias.detach().zero_()\n","        self.linear2bn=torch.nn.BatchNorm1d(hidden2)\n","        self.linear3=torch.nn.Linear(hidden2,hidden3)\n","        self.linear3.weight.detach().normal_(0.,.1)\n","        self.linear3.bias.detach().zero_()\n","        self.linear3bn=torch.nn.BatchNorm1d(hidden3)\n","        self.linear_out=torch.nn.Linear(hidden3,num_classes)\n","        self.linear_out.weight.detach().normal_(0.,.1)\n","        self.linear_out.bias.detach().zero_()        \n","    def forward(self,x):\n","        y=self.linear1(x); y=self.linear1bn(y)\n","        y=tnnf.relu(y)\n","        y=self.linear2(y); y=self.linear2bn(y)\n","        y=tnnf.relu(y)\n","        y=self.linear3(y); y=self.linear3bn(y)\n","        y=tnnf.relu(y)\n","        logits=self.linear_out(y)\n","        probs=tnnf.log_softmax(logits,dim=1)\n","        return logits,probs\n","torch.manual_seed(random_seed)\n","model=MLPN2(num_features=num_features2,\n","            num_classes=num_classes2)\n","model=model.to(dev); learning_rate=.001\n","optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jZ0H5RWSHPoA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0e60636b-cc5b-4447-df1d-3ddd020a7d68","executionInfo":{"status":"ok","timestamp":1591187424486,"user_tz":-180,"elapsed":371087,"user":{"displayName":"Olga Safu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhITqyZukwHZ9teMEwtxrx1LXmu7BQL_S_bK8qJFLU=s64","userId":"13149748190150435632"}}},"source":["%train_run2 30"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Epoch: 001/030 | Batch 000/177 | Cost: 4.0540\n","Epoch: 001/030 train accuracy: 4.06%\n","Epoch: 002/030 | Batch 000/177 | Cost: 3.6881\n","Epoch: 002/030 train accuracy: 4.98%\n","Epoch: 003/030 | Batch 000/177 | Cost: 3.6466\n","Epoch: 003/030 train accuracy: 6.20%\n","Epoch: 004/030 | Batch 000/177 | Cost: 3.5837\n","Epoch: 004/030 train accuracy: 7.39%\n","Epoch: 005/030 | Batch 000/177 | Cost: 3.2699\n","Epoch: 005/030 train accuracy: 8.09%\n","Epoch: 006/030 | Batch 000/177 | Cost: 3.4331\n","Epoch: 006/030 train accuracy: 9.35%\n","Epoch: 007/030 | Batch 000/177 | Cost: 3.4588\n","Epoch: 007/030 train accuracy: 10.62%\n","Epoch: 008/030 | Batch 000/177 | Cost: 3.1379\n","Epoch: 008/030 train accuracy: 11.22%\n","Epoch: 009/030 | Batch 000/177 | Cost: 3.3201\n","Epoch: 009/030 train accuracy: 12.53%\n","Epoch: 010/030 | Batch 000/177 | Cost: 3.1678\n","Epoch: 010/030 train accuracy: 13.66%\n","Epoch: 011/030 | Batch 000/177 | Cost: 3.0232\n","Epoch: 011/030 train accuracy: 14.62%\n","Epoch: 012/030 | Batch 000/177 | Cost: 3.0326\n","Epoch: 012/030 train accuracy: 15.48%\n","Epoch: 013/030 | Batch 000/177 | Cost: 2.9845\n","Epoch: 013/030 train accuracy: 16.16%\n","Epoch: 014/030 | Batch 000/177 | Cost: 3.1335\n","Epoch: 014/030 train accuracy: 16.98%\n","Epoch: 015/030 | Batch 000/177 | Cost: 2.9808\n","Epoch: 015/030 train accuracy: 18.21%\n","Epoch: 016/030 | Batch 000/177 | Cost: 3.1477\n","Epoch: 016/030 train accuracy: 19.30%\n","Epoch: 017/030 | Batch 000/177 | Cost: 2.8768\n","Epoch: 017/030 train accuracy: 19.53%\n","Epoch: 018/030 | Batch 000/177 | Cost: 2.9315\n","Epoch: 018/030 train accuracy: 20.35%\n","Epoch: 019/030 | Batch 000/177 | Cost: 2.9492\n","Epoch: 019/030 train accuracy: 21.41%\n","Epoch: 020/030 | Batch 000/177 | Cost: 2.8950\n","Epoch: 020/030 train accuracy: 22.25%\n","Epoch: 021/030 | Batch 000/177 | Cost: 2.7836\n","Epoch: 021/030 train accuracy: 23.12%\n","Epoch: 022/030 | Batch 000/177 | Cost: 2.6668\n","Epoch: 022/030 train accuracy: 23.36%\n","Epoch: 023/030 | Batch 000/177 | Cost: 2.7092\n","Epoch: 023/030 train accuracy: 24.14%\n","Epoch: 024/030 | Batch 000/177 | Cost: 2.7454\n","Epoch: 024/030 train accuracy: 25.11%\n","Epoch: 025/030 | Batch 000/177 | Cost: 2.5240\n","Epoch: 025/030 train accuracy: 25.53%\n","Epoch: 026/030 | Batch 000/177 | Cost: 2.6521\n","Epoch: 026/030 train accuracy: 26.27%\n","Epoch: 027/030 | Batch 000/177 | Cost: 2.6919\n","Epoch: 027/030 train accuracy: 27.10%\n","Epoch: 028/030 | Batch 000/177 | Cost: 2.7441\n","Epoch: 028/030 train accuracy: 27.80%\n","Epoch: 029/030 | Batch 000/177 | Cost: 2.5129\n","Epoch: 029/030 train accuracy: 28.07%\n","Epoch: 030/030 | Batch 000/177 | Cost: 2.7117\n","Epoch: 030/030 train accuracy: 28.68%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e1JxKb1kJUgb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"6baa125d-85ba-4892-9feb-6f6b42e4594a","executionInfo":{"status":"ok","timestamp":1591187430977,"user_tz":-180,"elapsed":6478,"user":{"displayName":"Olga Safu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhITqyZukwHZ9teMEwtxrx1LXmu7BQL_S_bK8qJFLU=s64","userId":"13149748190150435632"}}},"source":["%print_acc2 train\n","%print_acc2 test"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Train accuracy: 28.4884%\n","Test accuracy: 23.6082%\n"],"name":"stdout"}]}]}