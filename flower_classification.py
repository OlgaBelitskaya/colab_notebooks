# -*- coding: utf-8 -*-
"""flower_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H2ArWH_1kYfkIoCbxleX-aHAozRVBAdB

# üìë &nbsp; Deep Learning. P0: Image Classification

<a href='https://olgabelitskaya.github.io/README.html'>&#x1F300; &nbsp; GitHub Pages &nbsp; &nbsp; &nbsp;</a> 
<a href='https://olgabelitskaya.gitlab.io/index.html'>&#x1F300; &nbsp; GitLab Pages &nbsp; &nbsp; &nbsp;</a> 
<a href='https://www.instagram.com/olga.belitskaya/'>&#x1F300; &nbsp; Instagram Posts &nbsp; &nbsp; &nbsp;</a>  <a href='https://www.pinterest.ru/olga_belitskaya/code-style/'>&#x1F300; &nbsp; Pinterest Posts &nbsp; &nbsp; &nbsp;</a><br/>
In this project, we'll classify images from the `Flower Color Images`
<a href='https://www.kaggle.com/olgabelitskaya/flower-color-images'> 
Kaggle Dataset</a>.<br/>
The content is very simple:<br/> 
photo images (128x128x3) with 20 species of flowering plants<br/>
stored in the file of `Hierarchical Data Format` 
<a href='https://raw.githubusercontent.com/OlgaBelitskaya/data_kitchen/main/Flowers128.h5'>Flowers128.h5</a>.<br/>
In the original dataset, the photo files are in the `.png` format and the labels are integers.<br/>
We'll preprocess the images, then train a neural network on all the samples.
<br/> 
We are going to apply 
<a href='https://keras.io/'>Keras: The Python Deep Learning library</a>.<br/>
At the end, we'll get to see the neural network's predictions on the sample images.<br/>
## ‚úíÔ∏è &nbsp;Step 0. Modules & Versions
"""

from IPython.display import display,HTML
import pandas as pd,numpy as np,tensorflow as tf
import h5py,urllib,random,PIL.Image
import warnings; warnings.filterwarnings('ignore')
import seaborn as sn,pylab as pl
import tensorflow.keras.callbacks as tkc,\
tensorflow.keras.layers as tkl
from tensorflow.keras import __version__
print('keras version:', __version__)
print('tensorflow version:', tf.__version__)

"""## ‚úíÔ∏è &nbsp;Step 1. Loading and Exploring the Data """

file_path='https://raw.githubusercontent.com/'+\
          'OlgaBelitskaya/data_kitchen/main/'
file_name='Flowers128.h5'; cmap='autumn'
def h5py2data(file_path,file_name):
    input_file=urllib.request.urlopen(file_path+file_name)
    output_file=open(file_name,'wb'); 
    output_file.write(input_file.read())
    output_file.close(); input_file.close()
    with h5py.File(file_name,'r') as f:
        keys=list(f.keys())
        print('h5py.File keys: '+', '.join(keys))
        images=np.array(f[keys[0]])
        labels=np.array(f[keys[1]])
        names=[el.decode('utf-8') for el in f[keys[2]]]
        f.close()
    return images,labels,names
images,labels,names=h5py2data(file_path,file_name)
num_classes=len(names); img_size=128

#@title Creating Examples of Image Files
def html_table(names,start,end):
    colors=['#FF355E','#FF6037','#FF9966','#FFCC33','#FFFF66',
            '#CCFF00','#66FF66','#50BFE6','#FF6EFF','#FF00CC']
    html_str="""<style>
    @import 'https://fonts.googleapis.com/css?family=Akronim|Lobster';
    .label_table  {width:50%; background-color:silver;
                   font-family:Lobster; font-size:120%; 
                   border:double slategray; text-align:center;}
    .label_table th  {font-size:150%; border:double slategray;}
    .label_table td  {min-width:117px; border:double slategray;}             
    </style><table class='label_table'><tr>"""
    for j in range(end-start):
        html_str+="""<td style='color:
        """+colors[j]+"""'>"""+str(start+j)+"""</td>"""
    html_str+="""</tr><tr>"""
    for j in range(end-start):
        html_str+="""<td style='color:
        """+colors[j]+""";'>"""+names[start+j]+"""</td>"""
    html_str+="""</tr><tr>"""
    for j in range(end-start):
        html_str+="""<td style='color:
        """+colors[j]+""";'>"""+'%03d'%(start+j)+'.png'+"""</td>"""       
    html_str+="""</tr></table>"""
    return HTML(html_str)
sample=np.unique(labels,return_index=True)[1]
imgs=images[sample]
for i in range(num_classes):
    pl.imsave('%03d'%i+'.png',np.array(255*imgs[i],dtype='uint8'))
pil_imgs=[[PIL.Image.open('%03d'%i+'.png') 
           for i in range(i*num_classes//4,(i+1)*num_classes//4)]
          for i in range(4)]
width=sum([img.size[0] for img in pil_imgs[0]])
height=max([img.size[1] for img in pil_imgs[0]])
for i in range(4):
    stacked=PIL.Image.new(pil_imgs[i][0].mode,(width,height))
    x_pos=0
    for img in pil_imgs[i]:
        stacked.paste(img,(x_pos,0)); x_pos+=img.size[0]
    display(stacked)
    display(html_table(names,5*i,5*(i+1)))

"""## ‚úíÔ∏è &nbsp;Step 2. CSV Saving and Reloading the Data"""

images_csv=np.array(255*images,dtype='uint8')\
           .reshape(len(labels),128*128*3)
np.savetxt('flower_images.csv',images_csv,
           fmt='%i',delimiter=',')
np.savetxt('flower_labels.csv',labels,
           fmt='%i',delimiter=',')
images=pd.read_csv('flower_images.csv',header=None)
labels=pd.read_csv('flower_labels.csv',header=None)
display(images.iloc[:10,:20])
display(labels.iloc[:20].T)

images=np.array(images.values/255,dtype='float32')\
.reshape(len(labels),img_size,img_size,3)
labels=np.array(np.squeeze(labels.values),dtype='int32')
n=np.random.randint(len(labels))
st='label: '+names[labels[n]]
print(st); pl.figure(figsize=(4,3))
pl.imshow((images[n])); pl.show()

"""## ‚úíÔ∏è &nbsp;Step 3. Implement Preprocess Functions"""

def process_data(images,labels,names,cmap=cmap,
                  img_size=img_size,resize=False):
    N=images.shape[int(0)]; n=int(.1*N)
    if resize:
        images=tf.image.resize(images,[img_size,img_size]).numpy()
    shuffle_ids=np.arange(N)
    np.random.RandomState(12).shuffle(shuffle_ids)
    images=images[shuffle_ids]
    labels=labels[shuffle_ids] 
    x_test,x_valid,x_train=images[:n],images[n:2*n],images[2*n:]
    y_test,y_valid,y_train=labels[:n],labels[n:2*n],labels[2*n:]
    print('data outputs: \n')
    df=pd.DataFrame([[x_train.shape,x_valid.shape,x_test.shape],
                     [x_train.dtype,x_valid.dtype,x_test.dtype],
                     [y_train.shape,y_valid.shape,y_test.shape],
                     [y_train.dtype,y_valid.dtype,y_test.dtype]],
                    columns=['train','valid','test'],
                    index=['image shape','image type',
                           'label shape','label type'])
    display(df)
    print('distribution of labels: \n')
    idx=['labels','names']
    df=pd.DataFrame([labels,[names[l] for l in labels]],index=idx).T
    fig=pl.figure(figsize=(9,6))    
    for i in range(labels.shape[int(0)]):
        ax=fig.add_subplot(1,1,1)
        sn.countplot(y='names',data=df,palette=cmap,alpha=.5,ax=ax)
    pl.tight_layout(); pl.show()       
    return [x_train,x_valid,x_test,y_train,y_valid,y_test]
images,labels,names=h5py2data(file_path,file_name)
[x_train,x_valid,x_test,y_train,y_valid,y_test]=\
process_data(images,labels,names)

n=np.random.randint(len(x_train))
st='label: '+names[y_train[n]]
print(st); pl.figure(figsize=(4,3))
pl.imshow((x_train[n])); pl.show()

"""## ‚úíÔ∏è &nbsp;Step 4. Define the Model"""

def model(img_size=img_size,num_classes=num_classes):
    model=tf.keras.Sequential()
    model.add(tkl.Conv2D(
        32,(5,5),padding='same',input_shape=(img_size,img_size,3)))
    model.add(tkl.Activation('relu'))    
    model.add(tkl.MaxPooling2D(pool_size=(2,2)))
    model.add(tkl.Dropout(.25))
    model.add(tkl.Conv2D(96,(5,5)))
    model.add(tkl.Activation('relu'))   
    model.add(tkl.MaxPooling2D(pool_size=(2,2)))
    model.add(tkl.Dropout(.25))
    model.add(tkl.GlobalMaxPooling2D())
    model.add(tkl.Dense(1024,activation='tanh'))
    model.add(tkl.Dropout(.25))    
    model.add(tkl.Dense(64,activation='tanh'))
    model.add(tkl.Dropout(.25)) 
    model.add(tkl.Dense(num_classes))
    model.add(tkl.Activation('softmax'))
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='nadam',metrics=['accuracy'])   
    return model
model=model()

"""## ‚úíÔ∏è &nbsp;Step 5. Train the Model"""

early_stopping=tkc.EarlyStopping(
    monitor='val_loss',patience=20,verbose=2)
checkpointer=tkc.ModelCheckpoint(
    filepath='/tmp/checkpoint',verbose=int(2),save_weights_only=True,
    monitor='val_accuracy',mode='max',save_best_only=True)
lr_reduction=tkc.ReduceLROnPlateau(
    monitor='val_loss',verbose=2,patience=7,factor=.8)
history=model.fit(x_train,y_train,epochs=100,batch_size=16,
                  verbose=2,validation_data=(x_valid,y_valid),
                  callbacks=[checkpointer,early_stopping,lr_reduction])

def history_plot(fit_history):
    pl.figure(figsize=(10,10))
    pl.subplot(211)
    keys=list(fit_history.history.keys())[0:4]
    pl.plot(fit_history.history[keys[0]],
            color='slategray',label=keys[0])
    pl.plot(fit_history.history[keys[2]],
            color='crimson',label=keys[2])
    pl.xlabel('epochs'); pl.ylabel(keys[0])
    pl.legend(); pl.grid(); pl.tight_layout()    
    pl.subplot(212)
    pl.plot(fit_history.history[keys[1]],
            color='slategray',label=keys[1])
    pl.plot(fit_history.history[keys[3]],
            color='crimson',label=keys[3])
    pl.xlabel('epochs'); pl.ylabel('accuracy')    
    pl.legend(); pl.grid(); pl.tight_layout(); pl.show()
history_plot(history)

"""## ‚úíÔ∏è &nbsp;Step 6. Evaluate and Save the Model"""

model.load_weights('/tmp/checkpoint')
#model.save('model.h5')
#model=load_model('model.h5')
model.evaluate(x_test,y_test,verbose=0)

"""## ‚úíÔ∏è &nbsp;Step 7. Display Predictions


"""

y_test_predict=model.predict_classes(x_test)
fig=pl.figure(figsize=(14,6))
randch=np.random.choice(x_test.shape[0],size=10,replace=False)
for i,idx in enumerate(randch):
    ax=fig.add_subplot(2,5,i+1,xticks=[],yticks=[])
    ax.imshow(np.squeeze(x_test[idx]))
    pred_idx=y_test_predict[idx]
    true_idx=y_test[idx]
    col=('darkorange' if pred_idx==true_idx else 'darkred')
    ax.set_title('predict => {} \ntrue => {}'\
    .format(names[pred_idx],names[true_idx]),color=col)
pl.show()

"""## ‚úíÔ∏è &nbsp; Step 8. Keras Applications"""

from keras.applications.vgg16 \
import VGG16,preprocess_input as prei16
from tensorflow.keras.models import Model
vgg16bmodel=VGG16(weights='imagenet',include_top=False)
from keras.applications.inception_v3 \
import InceptionV3,preprocess_input as iv3pi
iv3bmodel=InceptionV3(weights='imagenet',include_top=False)

pvx_train=vgg16bmodel.predict(x_train)
pvx_valid=vgg16bmodel.predict(x_valid)
pvx_test=vgg16bmodel.predict(x_test)
input_shape=pvx_train.shape[1:]

def vgg16model(input_shape=input_shape,num_classes=num_classes):
    model=tf.keras.Sequential()  
    model.add(tkl.GlobalAveragePooling2D(input_shape=input_shape))   
    model.add(tkl.Dense(512))
    model.add(tkl.LeakyReLU(alpha=.02))
    model.add(tkl.Dropout(.5))        
    model.add(tkl.Dense(64))
    model.add(tkl.LeakyReLU(alpha=.02))
    model.add(tkl.Dropout(.25))   
    model.add(tkl.Dense(num_classes,activation='softmax'))    
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='nadam',metrics=['accuracy'])
    return model
vgg16model=vgg16model()

early_stopping=tkc.EarlyStopping(
    monitor='val_loss',patience=20,verbose=2)
checkpointer=tkc.ModelCheckpoint(
    filepath='/tmp/checkpoint',verbose=int(2),save_weights_only=True,
    monitor='val_accuracy',mode='max',save_best_only=True)
lr_reduction=tkc.ReduceLROnPlateau(
    monitor='val_loss',verbose=2,patience=7,factor=.8)
history=vgg16model.fit(
    pvx_train,y_train, validation_data=(pvx_valid,y_valid), 
    epochs=800,batch_size=64,verbose=2, 
    callbacks=[checkpointer,lr_reduction,early_stopping])

history_plot(history)
vgg16model.load_weights('/tmp/checkpoint')
vgg16model.evaluate(pvx_test,y_test,verbose=0)

x=iv3bmodel.output
x=tkl.GlobalAveragePooling2D()(x)
x=tkl.Dense(512)(x)
x=tkl.LeakyReLU(alpha=.02)(x)
y=tkl.Dense(num_classes,activation='softmax')(x)
iv3model=Model(inputs=iv3bmodel.input,outputs=y)
for layer in iv3bmodel.layers:
    layer.trainable=False    
iv3model.compile(optimizer='nadam',metrics=['accuracy'],
                 loss='sparse_categorical_crossentropy')

batch_size=64; steps,epochs=len(x_train)//batch_size,30
generator=tf.keras.preprocessing.image.ImageDataGenerator(
    shear_range=.2,zoom_range=.2,horizontal_flip=True)
checkpointer=tkc.ModelCheckpoint(
    filepath='/tmp/checkpoint',verbose=int(2),save_weights_only=True,
    monitor='val_accuracy',mode='max',save_best_only=True)
lr_reduction=tkc.ReduceLROnPlateau(
    monitor='val_loss',verbose=2,patience=7,factor=.8)
history=iv3model.fit_generator(
    generator.flow(x_train,y_train,batch_size=batch_size), 
    steps_per_epoch=steps,epochs=epochs, 
    callbacks=[checkpointer,lr_reduction],
    validation_data=(x_valid,y_valid))

for i,layer in enumerate(iv3bmodel.layers[173:]):
    print(i,layer.name)

for layer in iv3model.layers[:173]:
    layer.trainable=False
for layer in iv3model.layers[173:]:
    layer.trainable=True
iv3model.compile(optimizer='adam',metrics=['accuracy'],
                 loss='sparse_categorical_crossentropy')
history=iv3model.fit_generator(
    generator.flow(x_train,y_train,batch_size=64), 
    steps_per_epoch=steps,epochs=epochs, 
    callbacks=[checkpointer,lr_reduction],
    validation_data=(x_valid,y_valid))

iv3model.load_weights('/tmp/checkpoint')
scores=iv3model.evaluate(x_test,y_test)
print('accuracy: %.2f%%'%(scores[1]*100))

y_test_predict=iv3model.predict(x_test)
y_test_predict=[np.argmax(y_test_predict[i]) for i in range(len(y_test))]
y_test_predict=np.array(y_test_predict)

fig=pl.figure(figsize=(14,6))
randch=np.random.choice(x_test.shape[0],size=10,replace=False)
for i,idx in enumerate(randch):
    ax=fig.add_subplot(2,5,i+1,xticks=[],yticks=[])
    ax.imshow(np.squeeze(x_test[idx]))
    pred_idx=y_test_predict[idx]
    true_idx=y_test[idx]
    col=('darkorange' if pred_idx==true_idx else 'darkred')
    ax.set_title('predict => {} \ntrue => {}'\
    .format(names[pred_idx],names[true_idx]),color=col)
pl.show()